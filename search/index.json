[{"content":"梯度下降法推导 什么是梯度？ 梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。\n通俗来说，梯度就是表示某一函数在该点处的方向导数沿着该方向取得（模）较大值，即函数在当前位置的导数。笔者理解：在高维空间中参数 $\\boldsymbol{\\theta}$ 和 $f(\\boldsymbol{\\theta})$ 形成超曲面上，当 $\\boldsymbol{\\theta}$ 移动相同的 $\\lVert\\nabla\\boldsymbol{\\theta}\\lVert$ 模值（超距离）时，沿着梯度的方向，可以使得 $f(\\boldsymbol{\\theta})$ 变化量（增加）最大。梯度表示为如下： $$ \\nabla f(\\boldsymbol{\\theta})=\\frac{\\partial f(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} $$ 其中，$\\boldsymbol{\\theta}$为矢量（向量），大小为 $1 \\times n$；$f(\\boldsymbol{\\theta})$为标量，该函数称为目标函数，也叫做损失函数，有时候也用 $L$ 或者 $J$ 表示；$\\nabla f(\\boldsymbol{\\theta})$ 为矢量，大小为 $n \\times 1$。\n如果函数 $f(\\boldsymbol{\\theta})$ 是凸函数，那么就可以使用梯度下降算法进行优化。 $$ \\boldsymbol{\\theta} = \\boldsymbol{\\theta_0} - \\eta \\cdot \\nabla f(\\boldsymbol{\\theta_0}) $$ 其中，$\\boldsymbol{\\theta_0}$ 是自变量参数，代表当前参数位置坐标；$\\eta$ 是学习因子（学习率），用来调整下降时步进长度；$\\boldsymbol{\\theta}$ 是更新后的参数值，即通过一次梯度下降法之后的参数坐标位置。\n推导梯度下降法公式 我们对 $f(\\boldsymbol{\\theta})$ 进行一阶泰勒展开，如下： $$ f(\\boldsymbol{\\theta}) = f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) + o\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) $$ 注意，上面等式当 $f(\\boldsymbol{\\theta})$ 在 $\\boldsymbol{\\theta_{0}}$ 可导时，恒成立；其中，$ o\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) $ 为余项（或者称为线性近似误差）。而且，这里只有 $\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\rightarrow 0$时，$o\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)$ 才为无穷小量，此时，可以用前两项近似 $f(\\boldsymbol{\\theta})$ 值，所以如果 $\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)$ 太大，误差会比较大。\n那么，当我们使用 $f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 来近似 $f(\\boldsymbol{\\theta})$时，我们需要计算 $\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$，即为目标函数在当前参数位置 $f(\\boldsymbol{\\theta_{0}})$ 处的梯度。 $$ f(\\boldsymbol{\\theta}) \\approx f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) $$ 这里，$\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}$ 是一个微小量，我们令 $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\lVert$ 为 $\\eta$，$\\eta$ 为标量，令单位向量 $\\frac{\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}}{\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\rVert}$ 为 $\\boldsymbol{v}$ 表示，则 $$ \\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}} = \\eta \\boldsymbol{v} $$ 代入到上面的表达式后： $$ f(\\boldsymbol{\\theta}) \\approx f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\eta \\boldsymbol{v} \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) $$ 此处是重点！！! 局部下降的目的是希望每次 $\\boldsymbol{\\theta}$ 更新，都能让函数值 $f(\\boldsymbol{\\theta})$ 变小。也就是说，希望上式 $f(\\boldsymbol{\\theta})\u0026lt;f\\left(\\boldsymbol{\\theta_{0}}\\right)$。那么，希望下面式子每次更新都可以成立： $$ f(\\boldsymbol{\\theta})-f\\left(\\boldsymbol{\\theta_{0}}\\right) \\approx \\eta \\boldsymbol{v} \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\u0026lt;0 $$ 因为 $\\eta$ 为标量，且一般设定为正值，所以可以忽略，不等式等价于： $$ \\boldsymbol{v} \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\u0026lt;0 $$ 其中，$\\boldsymbol{v}$ 和 $\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 都是向量，$\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 是当前位置的梯度，$\\boldsymbol{v}$ 表示下一步前进的单位向量，是需要求解的, 从而就能根据 $\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}=\\eta \\boldsymbol{v}$ 确定 $\\boldsymbol{\\theta}$ 的值了。\n让我们来分析，当 $\\boldsymbol{v}$ 和 $\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 互为反向，即 $\\boldsymbol{v}$ 为当前梯度方向的负方向的时候，能让 $\\lvert \\boldsymbol{v} \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) \\rvert$ 最大，从而使得 $f(\\boldsymbol{\\theta})$ 最大程度地减小，也就保证了 $\\boldsymbol{v}$ 的方向是局部最大程度下降的方向。那么： $$ \\boldsymbol{v}=-\\frac{\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T}}{\\lVert\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\lVert} $$ 再根据 $\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}} = \\eta \\boldsymbol{v}$ 得到： $$ \\boldsymbol{\\theta} = \\boldsymbol{\\theta_{0}} - \\eta \\frac{\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T}}{\\lVert\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\lVert} $$ 一般地，因为 $\\lVert\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\lVert$ 是标量，可以并入到因子 $\\eta$ 中，即简化为 $$ \\boldsymbol{\\theta} = \\boldsymbol{\\theta_{0}} - \\eta^{\\prime} \\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} $$ 一般 $\\eta^{\\prime}$ 为学习率，是一个微小常量，通常取0.01，0.001，0.0001等值。\n梯度下降法分析 如果将 $f(\\boldsymbol{\\theta})$ 在 $\\boldsymbol{\\theta_{0}}$ 处进行二阶泰勒展开： $$ f(\\boldsymbol{\\theta}) = f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) + \\frac{1}{2}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)\\boldsymbol{H}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)^{T} + o\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) $$ 其中，$\\boldsymbol{H}$ 为Hessian矩阵，其展开形式为：\n$$ H=\\left(\\begin{array}{cccc} \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\end{array}\\right) $$\n我们将梯度下降法的迭代公式 $\\boldsymbol{\\theta} = \\boldsymbol{\\theta_{0}} - \\eta^{\\prime} \\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T}$ 代入到二阶泰勒展开式中，并且忽略二阶余项误差，得到： $$ f(\\boldsymbol{\\theta})-f\\left(\\boldsymbol{\\theta_{0}}\\right) \\approx -\\eta^{\\prime}\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\frac{1}{2} \\eta^{\\prime2} \\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\boldsymbol{H}\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) $$ 上面式子右边是梯度下降法一步迭代后引起目标函数 $f(\\boldsymbol{\\theta})$ 的变化量，每次迭代的变化量其实不一定是负的，使得 $f(\\boldsymbol{\\theta})$ 减小，也可能引起 $f(\\boldsymbol{\\theta})$ 增加。\n原因分析：\n理论分析来讲，当 $\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\rightarrow 0$时候，即当学习率 $\\eta^{\\prime}$ 足够小时，一阶展开余项 $o\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)$ 趋近于0，不会使得上式右边为负，也就可以保证迭代可以成功下降，但是，这样会导致达到极小值需要的迭代次数增加，使得学习过慢，增加计算和时间成本。 如果Hessian矩阵是负定的，可以保证 $\\frac{1}{2} \\eta^{\\prime2} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)^{T} \\boldsymbol{H}\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 项为负，那么也可以保证一次梯度下降迭代后，使得目标函数下降。 当学习率 $\\eta^{\\prime}$过大时，二阶余项误差变大，二阶泰勒展开的准确性也就不高了，因此可以用启发式寻找合适的学习率。【知识延伸】 下面对于 $\\frac{1}{2} \\eta^{\\prime2} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)^{T} \\boldsymbol{H}\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)$ 为正进行分析讨论。\n如果我们使上面式子右边小于0，即： $$ -\\eta^{\\prime}\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\frac{1}{2} \\eta^{\\prime2} \\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\boldsymbol{H}\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) \u0026lt; 0 $$ 整理 $\\eta^{\\prime}$ 得到： $$ \\eta^{\\prime} \u0026lt; \\frac{\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)}{\\left(\\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)\\right)^{T} \\boldsymbol{H} \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right)} $$ 那么，当学习率满足上面式子时，总可以梯度下降。不过上面式子也是在泰勒二阶展开下的近似分析。\n下面使用参数只有一维情况下的特例情况下，画图说明上面的情况：\n牛顿迭代法推导 通过上面泰勒展开式分析梯度下降法，我们可以知道梯度下降法仅使用梯度信息进行迭代下降，在大多数情况下效果还可以，并且由于目前神经网络中反向传播算法可以高效求解梯度，所以当前对于神经网络的优化普通采用梯度下降法。关于反向传播算法可以在我的神经网络之反向传播算法文章中了解。\n牛顿法迭代公式 二阶泰勒展开近似公式为： $$ f(\\boldsymbol{\\theta}) \\approx f\\left(\\boldsymbol{\\theta_{0}}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right) \\cdot \\nabla f\\left(\\boldsymbol{\\theta_{0}}\\right) + \\frac{1}{2}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)\\boldsymbol{H}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta_{0}}\\right)^{T} $$ 我们对式子右边求极值点，则可以得到：\n参考 梯度下降法的推导\n相信我你没真明白牛顿法与梯度下降法 优化理论——梯度下降法与牛顿法\n延伸阅读 论文 《A stochastic quasi-Newton method for large-scale optimization 》 《A Linearly-Convergent Stochastic L-BFGS Algorithm》 《A Multi-Batch L-BFGS Method for Machine Learning》 《Fast Exact Multiplication by the Hessian》 疑问 为什么神经网络每一层的学习率一样，应该是前面层学习率小，后面层学习率大。\n","date":"2022-06-23T11:17:44+08:00","permalink":"https://qiyueliuhuo.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/","title":"机器学习中的优化方法"},{"content":"前言 OpenMMLab学习资料：OpenMMLab 模块化设计背后的功臣\n对于mmdetection的学习，可以直接阅读官方的文档，现在官方提供了中文文档，非常详细，点击mmdetection中文文档阅读。\n关于基于mmdetection框架实现Faster R-CNN算法，可以先阅读官方在知乎上发布的文章，轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN，本篇博客是对其进行从源码实现层面进行补充解读。\nmmdetection代码框架 通过下面命令拉取最新的代码\n1 git clone git@github.com:open-mmlab/mmdetection.git 代码仓库主分支为master分支，目前计算机视觉领域中目标检测算法正在快速发展，mmdetection也不断地实现新的算法，最新实现的算法查看CHANGELOG。\n如果觉得master代码更新比较频繁，影响自己基于mmdetection的算法实现，也可以基于最新发布的版本创建自己的分支，如下命令：\n1 git checkout -b newbranch v2.25.0 # 截止2022.06.19发布的最新稳定版 目前还是推荐直接使用master代码，并且要经常 git pull 拉取最新的代码，这样最新的一些组件也会更新，方便基于最新的组件搭建自己的网络算法。\n之前阅读的YOLO v4论文中，作者将目标检测抽象为下面几个部分，mmdetection的代码也是这样抽象的：\n下面代码分析基于mmdetection的源码当前最近commit: ca11860f4f3c3ca2ce8340e2686eeaec05b29111 ，时间 2022.06.20。\n下面是mmdetection代码框架的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 . ├── CITATION.cff ├── LICENSE ├── MANIFEST.in ├── README.md ├── README_zh-CN.md ├── andy_README.md ├── configs # 存放所有的配置文件，可以自己写配置文件，也可以继承自_base_下的四种配置文件 ├── demo ├── docker ├── docs ├── mmdet # mmdeteetion的核心代码部分，其他工具都依赖于该部分的代码 ├── model-index.yml ├── pytest.ini ├── requirements ├── requirements.txt ├── resources ├── setup.cfg ├── setup.py ├── tests # 集成测试相关代码 └── tools # 提供训练、测试等工具代码 以下是configs目录下的配置文件说明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 configs ├── _base_ │ ├── datasets # 数据集加载，不同的数据集格式获取数据代码。 │ ├── default_runtime.py # 默认的运行时配置文件，主要配置包括权重保存频率、日志频率，日志等级等信息。 │ ├── models # 不同的目标检测模型配置文件 │ └── schedules # 各种训练策略配置文件 ├── albu_example # ...其他目录文件：1）继承自上面的四种文件，并做一些针对性修改；2）可以自己写配置文件，配置所有参数。 │ ├── README.md │ └── mask_rcnn_r50_fpn_albu_1x_coco.py ├── atss │ ├── README.md │ ├── atss_r101_fpn_1x_coco.py │ ├── atss_r50_fpn_1x_coco.py │ └── metafile.yml ├── autoassign 以下是mmdet（核心代码）目录下的代码说明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 mmdet ├── __init__.py # 判断配置的mmcv是否符合要求 ├── apis # 训练和测试相关依赖的函数，有随机种子生成、训练、单GPU测试、多GPU测试等 │ ├── __init__.py │ ├── inference.py │ ├── test.py │ └── train.py ├── core # 内核代码，包括锚框生成、边界框计算、结果评估、数据结构、掩码生成、可视化、钩子函数等等核心代码 │ ├── __init__.py │ ├── anchor │ ├── bbox │ ├── data_structures │ ├── evaluation │ ├── export │ ├── hook │ ├── mask │ ├── optimizers │ ├── post_processing │ ├── utils │ └── visualization ├── datasets # 数据加载器的具体实现，对应configs/datasets │ ├── __init__.py │ ├── api_wrappers │ ├── builder.py │ ├── cityscapes.py │ ├── coco.py │ ├── coco_panoptic.py │ ├── custom.py │ ├── dataset_wrappers.py │ ├── deepfashion.py │ ├── lvis.py │ ├── openimages.py │ ├── pipelines │ ├── samplers │ ├── utils.py │ ├── voc.py │ ├── wider_face.py │ └── xml_style.py ├── models # 不同模型的具体实现，分为不同的主干、颈部、头部、损失函数等等，对应 configs/models │ ├── __init__.py │ ├── backbones │ ├── builder.py │ ├── dense_heads │ ├── detectors │ ├── losses │ ├── necks │ ├── plugins │ ├── roi_heads │ ├── seg_heads │ └── utils ├── utils # 通用工具 │ ├── __init__.py │ ├── collect_env.py │ ├── compat_config.py │ ├── contextmanagers.py │ ├── logger.py │ ├── memory.py │ ├── misc.py │ ├── profiling.py │ ├── replace_cfg_vals.py │ ├── setup_env.py │ ├── split_batch.py │ ├── util_distribution.py │ ├── util_mixins.py │ └── util_random.py └── version.py # 记录mmdetection 的版本 以下是tools目录下的代码说明：\n1 2 3 4 5 6 7 8 9 10 11 12 tools ├── analysis_tools # 分析日志和预测效果 ├── dataset_converters # 数据集转换 ├── deployment # 部署工具 ├── dist_test.sh ├── dist_train.sh ├── misc # 杂项，下载数据集、打印配置信息等工具 ├── model_converters ├── slurm_test.sh ├── slurm_train.sh ├── test.py # 测试模型 └── train.py # 根据配置文件进行训练 Faster R-CNN 模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 data └── VOCdevkit ├── VOC2007 │ ├── Annotations │ ├── ImageSets │ ├── JPEGImages │ ├── SegmentationClass │ └── SegmentationObject └── VOC2012 ├── Annotations ├── ImageSets ├── JPEGImages ├── SegmentationClass └── SegmentationObject 根据官方文档，配置相关环境，下载VOC数据集（这里选择相对较小的VOC数据进行训练），并且组织好数据集目录结构，如上所示，然后就可以使用下面命令进行训练：\n1 python tools/train.py configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py 笔者训练得到的结果是：mAP 78.8，该结果和官方给的代码结果差将近两个点，可能是训练时候的参数设置不一样导致的。\nFaster R-CNN 模型测试 使用下面命令进行测试：\n1 python tools/test.py configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py checkpoints/faster_rcnn_r50_fpn_1x_voc0712_20220320_192712-54bef0f3.pth --eval mAP 这里的checkpoints目录下的模型文件是我下载mmdetection训练好的模型文件，可以替换成自己训练好的文件，进行测试。\nFaster R-CNN 配置文件解读 上面训练和测试使用的模型配置文件都是configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 _base_ = [ \u0026#39;../_base_/models/faster_rcnn_r50_fpn.py\u0026#39;, \u0026#39;../_base_/datasets/voc0712.py\u0026#39;, \u0026#39;../_base_/default_runtime.py\u0026#39; ] model = dict(roi_head=dict(bbox_head=dict(num_classes=20))) # 修改类别个数 # optimizer optimizer = dict(type=\u0026#39;SGD\u0026#39;, lr=0.01, momentum=0.9, weight_decay=0.0001) optimizer_config = dict(grad_clip=None) # 不进行梯度截断 # learning policy # actual epoch = 3 * 3 = 9 lr_config = dict(policy=\u0026#39;step\u0026#39;, step=[3]) # runtime settings runner = dict( type=\u0026#39;EpochBasedRunner\u0026#39;, max_epochs=4) # actual epoch = 4 * 3 = 12 可以看到上面的配置文件，继承自三个配置文件，下面解读configs/_base_/models/faster_rcnn_r50_fpn.py模型配置文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 # model settings model = dict( type=\u0026#39;FasterRCNN\u0026#39;, backbone=dict( type=\u0026#39;ResNet\u0026#39;, # 骨架网络类名 depth=50, # 表示使用 ResNet50 num_stages=4, # ResNet 系列包括 stem + 4个 stage 输出 # 表示本模块输出的特征图索引，(0, 1, 2, 3),表示4个 stage 输出都需要， # 其 stride 为 (4,8,16,32)，channel 为 (256, 512, 1024, 2048) out_indices=(0, 1, 2, 3), frozen_stages=1, # 表示固定 stem 加上第一个 stage 的权重，不进行训练 norm_cfg=dict(type=\u0026#39;BN\u0026#39;, requires_grad=True), # BN 层 norm_eval=True, # backbone 所有的 BN 层的均值和方差都直接采用全局预训练值，不进行更新 style=\u0026#39;pytorch\u0026#39;, # 默认采用 pytorch 模式 init_cfg=dict(type=\u0026#39;Pretrained\u0026#39;, checkpoint=\u0026#39;torchvision://resnet50\u0026#39;)), neck=dict( type=\u0026#39;FPN\u0026#39;, in_channels=[256, 512, 1024, 2048], # ResNet 模块输出的4个尺度特征图通道数 out_channels=256, # FPN 输出的每个尺度输出特征图通道 num_outs=5), # FPN 输出特征图个数，将最高层特征图再进行一个pooling操作，产生更高层特征图 rpn_head=dict( type=\u0026#39;RPNHead\u0026#39;, in_channels=256, # FPN 层输出特征图通道数 feat_channels=256, # 中间特征图通道数 anchor_generator=dict( type=\u0026#39;AnchorGenerator\u0026#39;, # 相当于 octave_base_scale，表示每个特征图的 base scales ??? # FPN 已经有多尺度操作，所以这里尺度只有一个8x8 scales=[8], # 然后通过控制不同的长宽比去产生不同比列的提议框 ratios=[0.5, 1.0, 2.0], # 每个特征图有 3 个高宽比例 strides=[4, 8, 16, 32, 64]), # 特征图对应的 stride，必须和特征图 stride 一致，不可以随意更改 ??? bbox_coder=dict( # 常用！ 对边界框回归的目标值进行一个编码 type=\u0026#39;DeltaXYWHBBoxCoder\u0026#39;, target_means=[.0, .0, .0, .0], target_stds=[1.0, 1.0, 1.0, 1.0]), loss_cls=dict( type=\u0026#39;CrossEntropyLoss\u0026#39;, use_sigmoid=True, loss_weight=1.0), loss_bbox=dict(type=\u0026#39;L1Loss\u0026#39;, loss_weight=1.0)), roi_head=dict( # roi_head会基于rpn_head产生的提议框，以及原图的特征图进行预测 type=\u0026#39;StandardRoIHead\u0026#39;, bbox_roi_extractor=dict( # 把提议框区域内的特征图从全图特征图中裁剪下来 type=\u0026#39;SingleRoIExtractor\u0026#39;, roi_layer=dict(type=\u0026#39;RoIAlign\u0026#39;, output_size=7, sampling_ratio=0), out_channels=256, featmap_strides=[4, 8, 16, 32]), bbox_head=dict( # 使用上面裁剪下来的特征送入 type=\u0026#39;Shared2FCBBoxHead\u0026#39;, # 2 个共享 FC 模块 in_channels=256, # 输入通道数，相等于 FPN 输出通道 fc_out_channels=1024, # 中间 FC 层节点个数 roi_feat_size=7, # RoIAlign 或 RoIPool 输出的特征图大小 num_classes=80, # 类别个数 bbox_coder=dict( # bbox 编解码策略？？？，除了参数外和 RPN 相同 type=\u0026#39;DeltaXYWHBBoxCoder\u0026#39;, target_means=[0., 0., 0., 0.], target_stds=[0.1, 0.1, 0.2, 0.2]), # 影响 bbox 分支的通道数，True 表示 4 通道输出，False 表示 4×num_classes 通道输出 reg_class_agnostic=False, loss_cls=dict( # 80类分类问题，没有使用sigmoid激活函数 type=\u0026#39;CrossEntropyLoss\u0026#39;, use_sigmoid=False, loss_weight=1.0), loss_bbox=dict(type=\u0026#39;L1Loss\u0026#39;, loss_weight=1.0))), # model training and testing settings train_cfg=dict( rpn=dict( assigner=dict( type=\u0026#39;MaxIoUAssigner\u0026#39;, # 最大 IoU 原则分配器 pos_iou_thr=0.7, # 正样本阈值 neg_iou_thr=0.3, # 负样本阈值 min_pos_iou=0.3, # 正样本阈值下限 ??? match_low_quality=True, ignore_iof_thr=-1), # 忽略 bboxes 的阈值，-1 表示不忽略 sampler=dict( type=\u0026#39;RandomSampler\u0026#39;, # 随机采样 num=256, # 产生1000个框，只采样256个进行训练，采样后每张图片的训练样本总数，不包括忽略样本 pos_fraction=0.5, # 正样本比例 neg_pos_ub=-1, # 正负样本比例，用于确定负样本采样个数上界 add_gt_as_proposals=False), # 是否加入 gt 作为 proposals 以增加高质量正样本数 allowed_border=-1, pos_weight=-1, debug=False), rpn_proposal=dict( nms_pre=2000, # nms之前 max_per_img=1000, # nms之后 nms=dict(type=\u0026#39;nms\u0026#39;, iou_threshold=0.7), min_bbox_size=0), rcnn=dict( assigner=dict( type=\u0026#39;MaxIoUAssigner\u0026#39;, pos_iou_thr=0.5, neg_iou_thr=0.5, min_pos_iou=0.5, match_low_quality=False, ignore_iof_thr=-1), sampler=dict( type=\u0026#39;RandomSampler\u0026#39;, num=512, pos_fraction=0.25, neg_pos_ub=-1, add_gt_as_proposals=True), pos_weight=-1, debug=False)), test_cfg=dict( rpn=dict( nms_pre=1000, max_per_img=1000, nms=dict(type=\u0026#39;nms\u0026#39;, iou_threshold=0.7), min_bbox_size=0), rcnn=dict( score_thr=0.05, nms=dict(type=\u0026#39;nms\u0026#39;, iou_threshold=0.5), max_per_img=100) # soft-nms is also supported for rcnn testing # e.g., nms=dict(type=\u0026#39;soft_nms\u0026#39;, iou_threshold=0.5, min_score=0.05) )) 下面对configs/_base_/datasets/voc0712.py数据集配置文件进行解读，其内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 _base_ = [ \u0026#39;../_base_/models/faster_rcnn_r50_fpn.py\u0026#39;, \u0026#39;../_base_/datasets/voc0712.py\u0026#39;, \u0026#39;../_base_/default_runtime.py\u0026#39; ] model = dict(roi_head=dict(bbox_head=dict(num_classes=20))) # 修改类别个数 data = dict( samples_per_gpu=8, # 修改GPU的batch_size，注意不能让其超过显存 workers_per_gpu=8, # 修改GPU的workers ) # optimizer optimizer = dict(type=\u0026#39;SGD\u0026#39;, lr=0.01, momentum=0.9, weight_decay=0.0001) optimizer_config = dict(grad_clip=None) # 不进行梯度截断 # learning policy # 有误 actual epoch = 3 * 3 = 9 lr_config = dict(policy=\u0026#39;step\u0026#39;, step=[8, 11]) # runtime settings runner = dict( type=\u0026#39;EpochBasedRunner\u0026#39;, max_epochs=12) # 有误 actual epoch = 4 * 3 = 12 笔者在源码基础上做了一些修改，将samples_per_gpu修改为8，workers_per_gpu修改为8，lr_config修改为dict(policy='step', step=[8, 11])，runner修改为dict(type='EpochBasedRunner', max_epochs=12)，注意实际训练时候batch_size为samples_per_gpu x gpus，我这里训练时候只用了一个GPU，所以batch_size为 8。\n训练代码解读 首先从tools里面的train.py入手，官方源码文件train.py。\n该python文件中main函数其实就做了四件事，这也是训练神经网络通用的步骤：\n设定和读取各种配置； 创建模型； 创建数据集； 将模型，数据集和配置传进训练函数，进行训练； 下面截取tools/train.py中main函数的代码片段进行解读，其内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def main(): # 第一件事 args = parse_args() cfg = Config.fromfile(args.config) ... # 省略部分代码，该部分代码对训练时上下文进行设置和校验 # 第二件事　创建模型 # 第一个参数cfg.model # 模型配置里面必须要有一个种类type，包括经典的算法如Faster RCNN, MaskRCNN等 # 其次，还包含几个部分，如backbone, neck, head # backbone有深度，stage等信息，如resnet50对应着3,4,6,3四个重复stages # neck一般FPN(feature pyramid network)，需要指定num_outs几个输出之类的信息（之后会看到） # head 就是具体到上层rpn_head,　shared_head，　bbox_head之类的 model = build_detector( cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg) model.init_weights() # 初始化模型参数 # 第三件事 datasets = [build_dataset(cfg.data.train)] if len(cfg.workflow) == 2: #是否添加验证集 val_dataset = copy.deepcopy(cfg.data.val) val_dataset.pipeline = cfg.data.train.pipeline datasets.append(build_dataset(val_dataset)) if cfg.checkpoint_config is not None: # save mmdet version, config file content and class names in # checkpoints as meta data cfg.checkpoint_config.meta = dict( mmdet_version=__version__ + get_git_hash()[:7], CLASSES=datasets[0].CLASSES) # add an attribute for visualization convenience model.CLASSES = datasets[0].CLASSES # 第四件事 train_detector( model, datasets, cfg, distributed=distributed, validate=args.validate, logger=logger) 让我们继续阅读build_detector函数的代码片段，其内容如下，该代码在mmdect/models/builder.py中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Copyright (c) OpenMMLab. All rights reserved. import warnings from mmcv.cnn import MODELS as MMCV_MODELS # mmdet中MODELS继承自该MMCV_MODELS from mmcv.utils import Registry MODELS = Registry(\u0026#39;models\u0026#39;, parent=MMCV_MODELS) BACKBONES = MODELS NECKS = MODELS ROI_EXTRACTORS = MODELS SHARED_HEADS = MODELS HEADS = MODELS LOSSES = MODELS DETECTORS = MODELS ... # 省略部分代码 def build_detector(cfg, train_cfg=None, test_cfg=None): \u0026#34;\u0026#34;\u0026#34;Build detector.\u0026#34;\u0026#34;\u0026#34; if train_cfg is not None or test_cfg is not None: warnings.warn( \u0026#39;train_cfg and test_cfg is deprecated, \u0026#39; \u0026#39;please specify them in model\u0026#39;, UserWarning) assert cfg.get(\u0026#39;train_cfg\u0026#39;) is None or train_cfg is None, \\ \u0026#39;train_cfg specified in both outer field and model field \u0026#39; assert cfg.get(\u0026#39;test_cfg\u0026#39;) is None or test_cfg is None, \\ \u0026#39;test_cfg specified in both outer field and model field \u0026#39; return DETECTORS.build( cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg)) 这里采用了Registry方式，即在mmdet/models/builder.py中实例化了一个Registry对象，该对象的父类是MMCV_MODELS，该类是mmcv中的一个类，其作用是用来管理多个模型，其中有一个build函数，该函数的作用是根据模型的配置信息，创建模型，并返回模型对象。\n下面代码来自mmcv，版本为1.5.2。\n从上面代码可以看到，mmdet中创建模型的函数build_model_from_cfg继承自mmcv/cnn/builder.py中的build_model_from_cfg函数，其内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Copyright (c) OpenMMLab. All rights reserved. from ..runner import Sequential from ..utils import Registry, build_from_cfg def build_model_from_cfg(cfg, registry, default_args=None): \u0026#34;\u0026#34;\u0026#34;Build a PyTorch model from config dict(s). Different from ``build_from_cfg``, if cfg is a list, a ``nn.Sequential`` will be built. Args: cfg (dict, list[dict]): The config of modules, is is either a config dict or a list of config dicts. If cfg is a list, a the built modules will be wrapped with ``nn.Sequential``. registry (:obj:`Registry`): A registry the module belongs to. default_args (dict, optional): Default arguments to build the module. Defaults to None. Returns: nn.Module: A built nn module. \u0026#34;\u0026#34;\u0026#34; if isinstance(cfg, list): modules = [ build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg ] return Sequential(*modules) else: return build_from_cfg(cfg, registry, default_args) MODELS = Registry(\u0026#39;model\u0026#39;, build_func=build_model_from_cfg) 下面我们在mmcv/utils/registry.py中查看build_from_cfg代码，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def build_from_cfg(cfg, registry, default_args=None): \u0026#34;\u0026#34;\u0026#34;Build a module from config dict when it is a class configuration, or call a function from config dict when it is a function configuration. Example: \u0026gt;\u0026gt;\u0026gt; MODELS = Registry(\u0026#39;models\u0026#39;) \u0026gt;\u0026gt;\u0026gt; @MODELS.register_module() \u0026gt;\u0026gt;\u0026gt; class ResNet: \u0026gt;\u0026gt;\u0026gt; pass \u0026gt;\u0026gt;\u0026gt; resnet = build_from_cfg(dict(type=\u0026#39;Resnet\u0026#39;), MODELS) \u0026gt;\u0026gt;\u0026gt; # Returns an instantiated object \u0026gt;\u0026gt;\u0026gt; @MODELS.register_module() \u0026gt;\u0026gt;\u0026gt; def resnet50(): \u0026gt;\u0026gt;\u0026gt; pass \u0026gt;\u0026gt;\u0026gt; resnet = build_from_cfg(dict(type=\u0026#39;resnet50\u0026#39;), MODELS) \u0026gt;\u0026gt;\u0026gt; # Return a result of the calling function Args: cfg (dict): Config dict. It should at least contain the key \u0026#34;type\u0026#34;. registry (:obj:`Registry`): The registry to search the type from. default_args (dict, optional): Default initialization arguments. Returns: object: The constructed object. \u0026#34;\u0026#34;\u0026#34; if not isinstance(cfg, dict): raise TypeError(f\u0026#39;cfg must be a dict, but got {type(cfg)}\u0026#39;) if \u0026#39;type\u0026#39; not in cfg: # type必须在配置文件里 if default_args is None or \u0026#39;type\u0026#39; not in default_args: raise KeyError( \u0026#39;`cfg` or `default_args` must contain the key \u0026#34;type\u0026#34;, \u0026#39; f\u0026#39;but got {cfg}\\n{default_args}\u0026#39;) if not isinstance(registry, Registry): raise TypeError(\u0026#39;registry must be an mmcv.Registry object, \u0026#39; f\u0026#39;but got {type(registry)}\u0026#39;) if not (isinstance(default_args, dict) or default_args is None): raise TypeError(\u0026#39;default_args must be a dict or None, \u0026#39; f\u0026#39;but got {type(default_args)}\u0026#39;) args = cfg.copy() if default_args is not None: for name, value in default_args.items(): args.setdefault(name, value) obj_type = args.pop(\u0026#39;type\u0026#39;) # 取出type的值 if isinstance(obj_type, str): obj_cls = registry.get(obj_type) #　从注册器里面取出type，注册过才能用 if obj_cls is None: raise KeyError( f\u0026#39;{obj_type} is not in the {registry.name} registry\u0026#39;) elif inspect.isclass(obj_type) or inspect.isfunction(obj_type): obj_cls = obj_type else: raise TypeError( f\u0026#39;type must be a str or valid type, but got {type(obj_type)}\u0026#39;) try: # 根据配置参数实例化模型，就是type定义的那个类（FasterRCNN）的实例化，返回了这个类的对象。 # 后面会再分析具体模型类（FasterRCNN）的定义。 return obj_cls(**args) except Exception as e: # Normal TypeError does not print class name. raise type(e)(f\u0026#39;{obj_cls.__name__}: {e}\u0026#39;) mmcv源码查看，到此为止。\n本篇博客分析Faster R-CNN，该算法在mmdet中模型注册type为FasterRCNN，我们找到该类的实现，位置为mmdet/models/detectors/faster_rcnn.py，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @DETECTORS.register_module() class FasterRCNN(TwoStageDetector): \u0026#34;\u0026#34;\u0026#34;Implementation of `Faster R-CNN \u0026lt;https://arxiv.org/abs/1506.01497\u0026gt;`_\u0026#34;\u0026#34;\u0026#34; def __init__(self, backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None, pretrained=None, init_cfg=None): super(FasterRCNN, self).__init__( backbone=backbone, neck=neck, rpn_head=rpn_head, roi_head=roi_head, train_cfg=train_cfg, test_cfg=test_cfg, pretrained=pretrained, init_cfg=init_cfg) 那么核心功能肯定都在TwoStageDetector里了。看代码之前首先回顾一下一个经典的双阶段检测Faster R-CNN的流程。\n下面我们来分析下/mmdet/models/detectors/two_stage.py文件，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 @DETECTORS.register_module() class TwoStageDetector(BaseDetector): \u0026#34;\u0026#34;\u0026#34;Base class for two-stage detectors. Two-stage detectors typically consisting of a region proposal network and a task-specific regression head. \u0026#34;\u0026#34;\u0026#34; def __init__(self, backbone, neck=None, rpn_head=None, roi_head=None, train_cfg=None, test_cfg=None, pretrained=None, init_cfg=None): super(TwoStageDetector, self).__init__(init_cfg) if pretrained: warnings.warn(\u0026#39;DeprecationWarning: pretrained is deprecated, \u0026#39; \u0026#39;please use \u0026#34;init_cfg\u0026#34; instead\u0026#39;) backbone.pretrained = pretrained self.backbone = build_backbone(backbone) if neck is not None: self.neck = build_neck(neck) if rpn_head is not None: rpn_train_cfg = train_cfg.rpn if train_cfg is not None else None rpn_head_ = rpn_head.copy() rpn_head_.update(train_cfg=rpn_train_cfg, test_cfg=test_cfg.rpn) self.rpn_head = build_head(rpn_head_) if roi_head is not None: # update train and test cfg here for now # TODO: refactor assigner \u0026amp; sampler rcnn_train_cfg = train_cfg.rcnn if train_cfg is not None else None roi_head.update(train_cfg=rcnn_train_cfg) roi_head.update(test_cfg=test_cfg.rcnn) roi_head.pretrained = pretrained self.roi_head = build_head(roi_head) self.train_cfg = train_cfg self.test_cfg = test_cfg @property def with_rpn(self): \u0026#34;\u0026#34;\u0026#34;bool: whether the detector has RPN\u0026#34;\u0026#34;\u0026#34; return hasattr(self, \u0026#39;rpn_head\u0026#39;) and self.rpn_head is not None @property def with_roi_head(self): \u0026#34;\u0026#34;\u0026#34;bool: whether the detector has a RoI head\u0026#34;\u0026#34;\u0026#34; return hasattr(self, \u0026#39;roi_head\u0026#39;) and self.roi_head is not None def extract_feat(self, img): \u0026#34;\u0026#34;\u0026#34;Directly extract features from the backbone+neck.\u0026#34;\u0026#34;\u0026#34; x = self.backbone(img) if self.with_neck: x = self.neck(x) return x def forward_dummy(self, img): \u0026#34;\u0026#34;\u0026#34;Used for computing network flops. See `mmdetection/tools/analysis_tools/get_flops.py` \u0026#34;\u0026#34;\u0026#34; outs = () # backbone x = self.extract_feat(img) # rpn if self.with_rpn: rpn_outs = self.rpn_head(x) outs = outs + (rpn_outs, ) proposals = torch.randn(1000, 4).to(img.device) # roi_head roi_outs = self.roi_head.forward_dummy(x, proposals) outs = outs + (roi_outs, ) return outs def forward_train(self, img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None, gt_masks=None, proposals=None, **kwargs): \u0026#34;\u0026#34;\u0026#34; Args: img (Tensor): of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. img_metas (list[dict]): list of image info dict where each dict has: \u0026#39;img_shape\u0026#39;, \u0026#39;scale_factor\u0026#39;, \u0026#39;flip\u0026#39;, and may also contain \u0026#39;filename\u0026#39;, \u0026#39;ori_shape\u0026#39;, \u0026#39;pad_shape\u0026#39;, and \u0026#39;img_norm_cfg\u0026#39;. For details on the values of these keys see `mmdet/datasets/pipelines/formatting.py:Collect`. gt_bboxes (list[Tensor]): Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. gt_labels (list[Tensor]): class indices corresponding to each box gt_bboxes_ignore (None | list[Tensor]): specify which bounding boxes can be ignored when computing the loss. gt_masks (None | Tensor) : true segmentation masks for each box used if the architecture supports a segmentation task. proposals : override rpn proposals with custom proposals. Use when `with_rpn` is False. Returns: dict[str, Tensor]: a dictionary of loss components \u0026#34;\u0026#34;\u0026#34; x = self.extract_feat(img) losses = dict() # RPN forward and loss if self.with_rpn: proposal_cfg = self.train_cfg.get(\u0026#39;rpn_proposal\u0026#39;, self.test_cfg.rpn) rpn_losses, proposal_list = self.rpn_head.forward_train( x, img_metas, gt_bboxes, gt_labels=None, gt_bboxes_ignore=gt_bboxes_ignore, proposal_cfg=proposal_cfg, **kwargs) losses.update(rpn_losses) else: proposal_list = proposals roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore, gt_masks, **kwargs) losses.update(roi_losses) return losses ... # 省略部分代码 关于上面代码，我画了一个示意图，可以方便理解两阶段检测的架构。 参考文章 入门mmdetection（壹）\n","date":"2022-06-18T17:14:15+08:00","permalink":"https://qiyueliuhuo.github.io/posts/%E5%9F%BA%E4%BA%8Emmdetection%E6%BA%90%E7%A0%81faster-r-cnn%E7%AE%97%E6%B3%95%E8%A7%A3%E8%AF%BB/","title":"基于mmdetection源码:Faster R-CNN算法解读"},{"content":"博客迁移到hugo，这是第一篇测试文章。\nhugo常用命令\n1 2 hugo new --kind post-bundle posts/测试文章 # 创建一个文章目录 hugo server -D # 启动服务器，显示草稿文章 ","date":"2022-06-03T17:22:13+08:00","permalink":"https://qiyueliuhuo.github.io/posts/my-first-hugo-post/","title":"My First Hugo Post"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![photo by florian klauer on unsplash](florian-klauer-nptlmg6jqdo-unsplash.jpg) ![photo by luca bravo on unsplash](luca-bravo-als7ewq41m8-unsplash.jpg) ![photo by helena hertz on unsplash](helena-hertz-wwzzxldpmog-unsplash.jpg) ![photo by hudai gayiran on unsplash](hudai-gayiran-3od_vkcdeaa-unsplash.jpg) 相册语法来自 typlog\n代码 1 2 3 #!/usr/bin/env python3 def main(): print(\u0026#34;hello, world!\u0026#34;) # 这是一个注释 图片居中显示 插入的图片前后留一行空行。\nmath $$ \\boldsymbol{H}=\\left(\\begin{array}{cccc} \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{1} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{2} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{1}} f(\\boldsymbol{\\theta}) \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{2}} f(\\boldsymbol{\\theta}) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2}}{\\partial \\theta_{n} \\partial \\theta_{n}} f(\\boldsymbol{\\theta}) \\end{array}\\right) $$\n当latex公式中存在换行时，需要用\\\\\\\\替代\\\\，\\\\ 会被 MarkDown 处理掉，具体查看katex 分段函数不能正常显示。\n","date":"2021-06-23T11:17:44+08:00","image":"https://qiyueliuhuo.github.io/posts/%E4%B8%BB%E9%A2%98%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://qiyueliuhuo.github.io/posts/%E4%B8%BB%E9%A2%98%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/","title":"主题测试文章"},{"content":"前言 如何评价一个算法模型的检测结果是好还是不好呢？\n检测结果的正确/错误类型 正确结果(True Positive)：算法检测到了某类物体 (Positive)，图中也确实有这个物体，检测结果正确(True)。（个人理解，这里其实成为正阳性更准确） 假阳性(False Positive)：算法检测到了某类物体 (Positive)，但图中其实没有这个物体，检测结果错误 (False)。通常我们把它也称为误检。 假阴性 (False Negative)：算法没有检测到物体 (Negative)，但图中其实有某类物体，检测结果错误(False)。通常，我们把它也称为漏检。 这里检测到的衡量标准：对于某个检测框，图中存在同类型的真值框且与之交并比大于阈值（通常取0.5） 例子： 上面检测有 7 张图像，其中绿色边界框表示 15 个真值框，红色边界框表示 24 个检测框。每个检测到的对象都有一个置信度，并由一个字母 (A,B,\u0026hellip;,Y) 标识。\n上表显示了边界框及其相应的置信度。最后一列将检测标识为 TP 或 FP。在这个例子中，如果 IOU 大于30% 则认为 TP，否则为 FP。通过查看上面的图像，我们可以大致判断检测是 TP 还是 FP\n更具体的分析，可以参考 目标检测指标\n目标检测评价指标 召回率、准确率 真值框总数与检测算法无关，因此只需将检测结果区分为TP和FP即可计算 recall 和 precision\n两种极端情况：\n检测器将所有锚框都判断为物体：召回率≈100%，但大量背景框预测为物体，FP很高，准确率很低； 检测器只输出确信度最高的1个检测框：以很大概率检测正确，准确率=100%，但因为大量物体被预测为背景，FN很高，召回率很低。 理想情况： 一个完美的检测器应该有100%召回率和100%的准确率；在算法能力有限的情况下，应该平衡二者。\n通常做法： 将检测框按置信度排序，仅输出置信度大于某个阈值的若干个框。\nAP（Average Precision） 为得到阈值无关的评分，可以遍历阈值，并对 Precision 和 Recall 求平均。\n具体做法：\n检测框按置信度排序，取前K个框计算 Precision 和 Recall。 遍历K从1至全部检测框，将得到的 Precision 和 Recall 值绘制在坐标系上，得到 PR 曲线。 定义 Average Precision = Precision 对 Recall 的平均值,即 PR 曲线下的面积,作为检测器的性能衡量指标。 笔者觉得这个指标很不科学！\n原因是：一个好的检测器只需要在某个置信度阈值下能够检测出所有真实的目标，并且没有漏检和误检，那么就可以确定该检测器非常好，而不需要在其他置信度阈值下都表现好。\nMean AP 分类别统计AP，并按类别平均即得到 Mean AP。\n部分数据集（如 COCO)还要求在不同的 loU (上面提到的IOU 大于30% )阈值下计算 Mean AP 并平均，作为最终评分；从0.5 ~ 0.05，每个0.05都会取一个IoU的阈值，计算 Mean AP，最终将这几个值进行平均，作为最终的一个评分。\n可衡量检测器在不同定位精度要求下的性能。\n","date":"2021-06-21T10:45:26+08:00","permalink":"https://qiyueliuhuo.github.io/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/","title":"目标检测模型的评估方法"},{"content":"Module nn.BNReLU2d 源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class BNReLU2d(nnq.BatchNorm2d): r\u0026#34;\u0026#34;\u0026#34; A BNReLU2d module is a fused module of BatchNorm2d and ReLU We adopt the same interface as :class:`torch.nn.quantized.BatchNorm2d`. Attributes: Same as torch.nn.quantized.BatchNorm2d \u0026#34;\u0026#34;\u0026#34; _FLOAT_MODULE = torch.nn.intrinsic.BNReLU2d def __init__(self, num_features, eps=1e-5, momentum=0.1): super(BNReLU2d, self).__init__(num_features, eps=eps, momentum=momentum) def forward(self, input): # Temporarily using len(shape) instead of ndim due to JIT issue # https://github.com/pytorch/pytorch/issues/23890 if len(input.shape) != 4: raise ValueError(\u0026#34;Input shape must be `(N, C, H, W)`!\u0026#34;) return torch.ops.quantized.batch_norm2d_relu( input, self.weight, self.bias, self.running_mean, self.running_var, self.eps, self.scale, self.zero_point) def _get_name(self): return \u0026#39;QuantizedBNReLU2d\u0026#39; @classmethod def from_float(cls, mod): # TODO: Add qat support for BNReLU2d return super(BNReLU2d, cls).from_float(mod)) nn.BatchNorm2d 1 2 3 4 5 class BatchNorm2d(_BatchNorm): def _check_input_dim(self, input): if input.dim() != 4: raise ValueError(\u0026#39;expected 4D input (got {}D input)\u0026#39; .format(input.dim())) torch.nn.BatchNorm2类如上面源码所示，该类具体实现部分基本上都来自继承_BatchNorm这个内部类，而后者类里面具体实现不是用python实现的，而是用c++、cuda实现的，这里我就不具体分析底层源码了。\n下面分析这个类功能：\n这篇论文中Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift提出了BN层。首先该类的输入为小批量带通道的二维输入，也就是输入的大小为(N,C,H,W)。对输入的数据做如下公式的变换： $$ y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta $$ 平均值和标准差是按小批量的每个维度计算，$\\gamma$和$\\beta$为通道维度上的可学习的参数向量，默认情况下，$\\gamma$为1，$\\beta$为0。\n","date":"2021-03-14T15:53:39Z","permalink":"https://qiyueliuhuo.github.io/posts/pytorch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E4%B8%89/","title":"Pytorch源码阅读(三)：BatchNorm Module"},{"content":" 注：笔者阅读的pytorch版本为1.7.0，torchvision版本为0.6\n前言 在这篇博客文章中，我主要来写关于pytorch中transforms模块，该模块提供了对图像各种预处理方法，位于torchvision/transforms/transforms.py，这些方法会应用在模型训练推理前，对图像进行预处理，再将处理后的图像送进深度网络中训练与推理。我想写这篇文章对这些预处理方法，进行学习理解，尽可能从源码角度，形象直观地展示这些图像预处理是如何对图像进行转换的。\ntransforms ： 读[trænsˈfɔːm]，变换、转换、移动。 eg. Fourier transform 傅里叶变换； It was an event that would transform my life. 那是能够彻底改变我一生的一件事。\ntransforms 模块相关源码分析 transforms.Compose类 先来看下transforms.Compose在实际代码中是如何运用的，下面代码是使用pytorch中的Dataset方式定义ImageNet数据集，也就说ImageNet继承自data.Dataset。\n1 2 3 4 5 6 7 train_dataset = torchvision.datasets.ImageNet(train_path, transform=transforms.Compose([ transforms.Resize((32, 32)), # 将图片缩放到指定大小（h,w）或者保持长宽比并缩放最短的边到int大小 transforms.CenterCrop(32), transforms.ToTensor()]) ) 从上面代码可以看出来transforms模块定义的对象，作为参数传入给ImageNet，在《pytorch源码(一)》中，了解到，通过for循环可以遍历Dataset对象获取图像数据，这篇文章介绍的transforms模块定义的类，一般在遍历Dataset获取图像前对图像进行预处理，那么通过for循环得到的图像就是进行处理后的图像。\n下面来分析transforms.Compose源码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Compose(object): \u0026#34;\u0026#34;\u0026#34;Composes several transforms together. Args: transforms (list of ``Transform`` objects): list of transforms to compose. Example: \u0026gt;\u0026gt;\u0026gt; transforms.Compose([ \u0026gt;\u0026gt;\u0026gt; transforms.CenterCrop(10), \u0026gt;\u0026gt;\u0026gt; transforms.ToTensor(), \u0026gt;\u0026gt;\u0026gt; ]) \u0026#34;\u0026#34;\u0026#34; def __init__(self, transforms): self.transforms = transforms def __call__(self, img): for t in self.transforms: img = t(img) return img def __repr__(self): format_string = self.__class__.__name__ + \u0026#39;(\u0026#39; for t in self.transforms: format_string += \u0026#39;\\n\u0026#39; format_string += \u0026#39; {0}\u0026#39;.format(t) format_string += \u0026#39;\\n)\u0026#39; return format_string Compose是一个容器，它是对多个transforms模块定义转换对象transform组合，本质上是对列表的包装（装饰模式？）。\n这里我将这些转换对象类定义为transform，包括transforms.CenterCrop、transforms.ToTensor等等。\n该类的构造器(constructor)函数入参为列表类型，列表里是多个transform转换对象；在__call__中，通过for循环遍历transforms列表，对图像依次进行调用t(img)，可以看到每个transform都是一个可调用对象，通过依次调用这些transform，对图像进行预处理，这些处理是按照列表顺序处理。注：list是一种有序的集合。\ntransforms.Resize类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Resize(object): \u0026#34;\u0026#34;\u0026#34;Resize the input PIL Image to the given size. Args: size (sequence or int): Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height \u0026gt; width, then image will be rescaled to (size * height / width, size) interpolation (int, optional): Desired interpolation. Default is ``PIL.Image.BILINEAR`` \u0026#34;\u0026#34;\u0026#34; def __init__(self, size, interpolation=Image.BILINEAR): assert isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2) self.size = size self.interpolation = interpolation def __call__(self, img): \u0026#34;\u0026#34;\u0026#34; Args: img (PIL Image): Image to be scaled. Returns: PIL Image: Rescaled image. \u0026#34;\u0026#34;\u0026#34; return F.resize(img, self.size, self.interpolation) def __repr__(self): interpolate_str = _pil_interpolation_to_str[self.interpolation] return self.__class__.__name__ + \u0026#39;(size={0}, interpolation={1})\u0026#39;.format(self.size, interpolate_str) 先分析Resize的构造器方法，参数size，为转换后的图像像素大小，如果size参数是这样的序列（h，w），输出大小将与此匹配。如果size是int，图像的较小边等于此数字。如果height \u0026gt; width，则图像将被重新缩放到$\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)$。另一个参数为interpolation，翻译为插值，默认值为双线性插值，这里我猜测下，当需要将图像分辨率变大，比如300*400提高至600*500分辨率，因为分辨率乘积代表像素点的个数，分辨率增大，那么原有的图像像素点不够，需要通过插值的方式新生成一部分像素点，这个参数就是控制如何产生这部分的像素点，在后面分析源码我会详细地说明插值方法。\n分析__call__方法，它的内部的实现直接调用了torchvision/transforms/functional.py模块中的方法，functional.py模块包含了很多对图像转换的具体方法实现，比如resize(img, size, interpolation=Image.BILINEAR)方法的具体实现等等。\n下面我使用Resize通过代码将475 * 300大小图片转换为237 * 150大小的图片。\n1 2 3 4 5 6 7 8 from torchvision.transforms import transforms from PIL import Image im = Image.open(\u0026#34;resources/dog-4671215_1280.jpg\u0026#34;) resize = transforms.Resize((150, 237)) im = resize(im) im.show() im.save(\u0026#39;resources/dog-150_237.jpg\u0026#39;) 缩小为 下面我来具体分析functional.py模块中的resize方法，源码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def resize(img, size, interpolation=Image.BILINEAR): r\u0026#34;\u0026#34;\u0026#34;Resize the input PIL Image to the given size. Args: img (PIL Image): Image to be resized. size (sequence or int): Desired output size. If size is a sequence like (h, w), the output size will be matched to this. If size is an int, the smaller edge of the image will be matched to this number maintaing the aspect ratio. i.e, if height \u0026gt; width, then image will be rescaled to :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)` interpolation (int, optional): Desired interpolation. Default is ``PIL.Image.BILINEAR`` Returns: PIL Image: Resized image. \u0026#34;\u0026#34;\u0026#34; if not _is_pil_image(img): raise TypeError(\u0026#39;img should be PIL Image. Got {}\u0026#39;.format(type(img))) if not (isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)): raise TypeError(\u0026#39;Got inappropriate size arg: {}\u0026#39;.format(size)) if isinstance(size, int): w, h = img.size if (w \u0026lt;= h and w == size) or (h \u0026lt;= w and h == size): return img if w \u0026lt; h: ow = size oh = int(size * h / w) return img.resize((ow, oh), interpolation) else: oh = size ow = int(size * w / h) return img.resize((ow, oh), interpolation) else: return img.resize(size[::-1], interpolation) resize函数是定义在functional.py模块中的一个函数，前两个if语句，用来做参数类型校验，主要用到python中内置的isinstance(x, A_tuple)函数，返回对象是类的实例还是子类的实例，值得注意的是，第二个参数，既可以是一个类对象，也可以是一个包含多个类对象的元组。关于Iterable的理解可以查看这篇博客理解Python的Iterable和Iterator。\n其他部分的代码也很容易理解，isinstance(size, int)为True时，size为图像较小的边的输出大小,输出图像另一边的大小根据原图的比例计算得出。\n最后的else分支也很容易理解，在这里可以学习到的一点是，如何取得一个列表倒序的结果？这里用了切片的方式size[::-1]，来将入参是传入的(h, w)逆序变成(w, h)，再传入PIL.Image对象的resize()方法，也就是说pytorch本身没有自己实现对图像的resize方法，而是在底层调用的PIL图像库的方法。\n下面先插入一些对python中切片的学习，不知道为什么我每次遇到切片表达方式都要重新查一遍各种切片具体是代表的什么含义（😖）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Last login: Wed Mar 10 14:19:12 on ttys001 \u0026gt;\u0026gt;\u0026gt; L = list(range(10)) \u0026gt;\u0026gt;\u0026gt; L [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u0026gt;\u0026gt;\u0026gt; L[::-1] # 倒序排列并且每1个取一个 [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] \u0026gt;\u0026gt;\u0026gt; L[::-2] # 倒序排列并且每2个取一个 [9, 7, 5, 3, 1] \u0026gt;\u0026gt;\u0026gt; L[::2] # 正序排列并且每2个取一个 [0, 2, 4, 6, 8] \u0026gt;\u0026gt;\u0026gt; L[:] # 原样复制 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u0026gt;\u0026gt;\u0026gt; L[1:5] # 取第一个到第五个 [1, 2, 3, 4] 参考：python切片\n至于PIL.Image对象的resize()方法具体分析，内部实现是比较复杂的，先暂时省略，后面再补(🤦)‍️。\ntransforms.CenterCrop类 CenterCrop类的功能是依据给定的size从中心裁剪，在pytorch中的实现非常简单，这里就不贴出来源码了，简单看下它的构造器方法，参数size为(h, w)，代表裁剪后的图像分辨率大小，而如果size为int型，那么裁剪为(size, size)大小的正方形中心图像。例如，下面在475 * 300大小图像的中心部分裁剪出120 * 120大小的图像： 裁剪出 transforms.ToTensor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class ToTensor(object): \u0026#34;\u0026#34;\u0026#34;Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8 In the other cases, tensors are returned without scaling. \u0026#34;\u0026#34;\u0026#34; def __call__(self, pic): \u0026#34;\u0026#34;\u0026#34; Args: pic (PIL Image or numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image. \u0026#34;\u0026#34;\u0026#34; return F.to_tensor(pic) def __repr__(self): return self.__class__.__name__ + \u0026#39;()\u0026#39; ToTensor从类名字就可以看出来这个类主要将PIL.Image对象和numpy.ndarray对象转换为torch.FloatTensor对象，值得注意的是，最终的转换得到的Tensor对象的size是(C x H x W)，通道数在前面，也就是说pytorch在处理图像时，一般情况下，将三维的图像排列是(通道数，高度，宽度)。另外，该类还会将原本的0~255的RGB三原色强度值进行归一化处理，统一除以255，使得值在0.0~1.0之间。\n","date":"2021-03-11T14:15:04Z","permalink":"https://qiyueliuhuo.github.io/posts/pytorch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E4%BA%8C/","title":"Pytorch源码阅读(二)：transforms模块"},{"content":" vue学习记录。\n注：\n学习目录在Workplace/vueLearning。 学完要综合学过前端和后端知识，开发一个完整的应用，综合应用知识。 使用的开发IDE为WebStorm。 一、邂逅Vuejs 认识Vuejs Vue在国内相比于React使用更加广泛、更火。\n三大框架：VueJs、AngularJs、React。 Vue：读音/vju:/，类似于view。 Vue是一个渐进式框架：\nVue可以作为你应用的一部分嵌入其中。（一点一点渐进式引入Vue） 也可以直接使用Vue核心库及其生态系统。（Vue全家桶） Vue的特点和高级功能：（目前不太了解，慢慢学习，慢慢体会）\n解耦视图和数据 可复用的组件 前端路由技术 状态管理 虚拟DOM 学习Vue前提：\n可以从零学习，不需要其他框架的知识为前提。 需要具备一定的HTML、CSS、JavaScript基础。 (JS※ 、ES6※。以后都会用ES6） 另： xcode 读音：叉code。 Vue不要读成v、u、e。 Vuejs安装方式 方式一：直接CDN引入 方式二：下载和引入 另： - 直接下载，拷贝到项目里面就可以使用，只有一个vue.js文件，hin方便。\n方式三：NPM安装 后续通过webpack和CLI的使用，然后再使用这种方式。 Vuejs初体验 script引入vue并使用： 修改数据：1. 界面不用改，声明式开发。 2. 响应式：当数据发生改变，界面自动改变。 创建Vue对象时候，传入参数含义：\n{}中包含了el属性：该属性决定了这个vue对象挂载到哪一个元素。 {}中包含了data属性:该属性中通常会存储一些数据。 数据可以是我们自己定义的。 也可以是来自网络，从服务器加载到的。 列表展示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;!--{{movies}} --\u0026gt; \u0026lt;!-- 这种会展示成json格式数据--\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li v-for=\u0026#34;item in movies\u0026#34;\u0026gt;{{item}}\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;../js/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const app = new Vue({ el: \u0026#39;#app\u0026#39;, data: { message: \u0026#39;你好呀\u0026#39;, movies: [\u0026#39;星际穿越\u0026#39;, \u0026#39;大话西游\u0026#39;, \u0026#39;盗墓空间\u0026#39;, \u0026#39;切尔诺贝利\u0026#39;] } }) \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Vue计数器案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;当前计数：{{counter}}\u0026lt;/h2\u0026gt; \u0026lt;button v-on:click=\u0026#34;add\u0026#34;\u0026gt;+\u0026lt;/button\u0026gt; \u0026lt;button v-on:click=\u0026#34;sub\u0026#34;\u0026gt;-\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;../js/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const app = new Vue({ el: \u0026#34;#app\u0026#34;, data: { counter: 0 }, methods: { add: function () { console.log(\u0026#39;add被执行\u0026#39;); this.counter++ }, sub: function () { console.log(\u0026#39;sub被执行\u0026#39;); this.counter-- } } }) \u0026lt;/script\u0026gt; 另： ES6不再使用var定义变量，而是使用let，let定义变量，const定义常量。 可以在游览器console中修改数据，会自动响应式展示。 @click是 v-on:click的语法糖。 Vuejs的MVVM 什么是MVVM：维基百科MVVM。 （需要再详细阅读） Vue的MVVM：图不清楚，需要重新找。 创建Vue实例传入的options el: string | HTML Element data: Object | Function (组件中data必须是一个函数) methods: { [key: string]: Function } （后面换成ES6的语法来写） 另： 开发中什么叫方法，什么称之为函数？ 答：都是通过function定义，类里面叫方法，定义在外面叫函数。 方法：method 函数：function Vue的生命周期（简单理解下） 另： 常用的生命周期函数：created、mounted。 二、Vue基础语法 代码规范 前端开发，缩进两个空格更加合适。 重复代码建立模板 将重复输入的内容设置到模板当中，在HTML（需要设置）中直接输入vue，点击回车，就可以自动填充模板中的内容。\n插值语法 Mustache语法 （也就是双大括号，用的最多）※ 1 2 3 4 5 6 7 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;!-- mustache语法中，不仅仅可以直接写变量，也可以写一些简单的表达式--\u0026gt; \u0026lt;h2\u0026gt;{{firstName + \u0026#39; \u0026#39; + lastName}}\u0026lt;/h2\u0026gt; \u0026lt;h2\u0026gt;{{firstName}} {{lastName}}\u0026lt;/h2\u0026gt; \u0026lt;h2\u0026gt;{{counter * 2}}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; v-once指令的使用 只展示一次，后面网页界面内容不会随着message值的改变而改变。也就是第二个h2标签显示的内容不会随着vue的app里面数据改变。\n一般情况下不用，特殊情况下需要使用。\n1 2 3 4 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;h2 v-once\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; v-html指令的使用 数据以HTML形式展示。 v-text指令的使用 （一般不用，没有双括号灵活） v-pre指令的使用 （很少使用） 将v-pre设置的标签内容原封不动展示，不进行解析。\n1 2 3 4 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;h2 v-pre\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; v-cloak指令的使用 （基本不用） cloak: 斗篷。 clock: 时钟。\n和css样式结合使用，可以实现当JS脚本未执行完，不显示内容，而不是显示一些未渲染的“乱码”，对用户友好。\nv-bind v-bind之前都是将数据插到内容中，而这个是将值插到标签的属性中，比如img标签的src属性，改变这个属性值。又比如a标签的href属性。不能用双括号语法操作，双括号是在标签内容中使用的。\n作用：动态绑定标签属性。\nv-bind基本使用 v-bind语法糖 （经常用）※ 将v-bind:简写成:。\nv-bind动态绑定class属性 对象语法：class = \u0026quot;\u0026quot; 双引号里面是个对象。 （经常用）※ 数组语法 （很少用） 1 2 3 \u0026lt;h2 class=\u0026#34;title\u0026#34; :class=\u0026#34;[active, \u0026#39;line\u0026#39;]\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;!-- active是个变量，定义在data中--\u0026gt; v-bind动态绑定style 对象语法：:style=\u0026quot;{key(属性名): value(属性值)}\u0026quot; 双引号里面是个对象。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;!-- \u0026lt;h2 :style=\u0026#34;{key(属性名): value(属性值)}\u0026#34;\u0026gt;{{message}}\u0026lt;/h2\u0026gt;--\u0026gt; \u0026lt;h2 :style=\u0026#34;{fontSize: finalSize, color: finalColor}\u0026#34;\u0026gt;{{message}}\u0026lt;/h2\u0026gt; \u0026lt;!-- key不需要加单引号，但是value要加，否则20px当作成变量了 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;../js/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const app = new Vue({ el: \u0026#34;#app\u0026#34;, data: { message: \u0026#39;你好呀\u0026#39;, finalSize: \u0026#39;100px\u0026#39;, finalColor: \u0026#39;red\u0026#39; } }) \u0026lt;/script\u0026gt; 数组语法：\u0026lt;div v-bind: style=\u0026quot;[basestyles, overridingstyles]\u0026quot;\u0026gt;/div\u0026gt; ，basestyles和overridingstyles为对象 （很少用） 计算属性 computed（很重要）※ 计算属性的使用 数据展示之前需要经过一定的处理，computed里面也是定义的函数，但是不要命名为动词，一般命名为名词。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;!-- \u0026lt;h2\u0026gt;{{firstName + \u0026#39; \u0026#39; + lastName}}\u0026lt;/h2\u0026gt;--\u0026gt; \u0026lt;!-- \u0026lt;h2\u0026gt;{{fullName()}}\u0026lt;/h2\u0026gt;--\u0026gt; \u0026lt;h2\u0026gt;{{fullName}}\u0026lt;/h2\u0026gt; \u0026lt;!-- 不需要加函数的括号 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;../js/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const app = new Vue({ el: \u0026#34;#app\u0026#34;, data: { message: \u0026#39;你好呀\u0026#39;, firstName: \u0026#39;lebron\u0026#39;, lastName: \u0026#39;James\u0026#39; }, computed: { fullName: function () { return this.firstName + \u0026#39; \u0026#39; + this.lastName + \u0026#39; 根据computed得到的\u0026#39;; } }, methods: { // fullName: function () { // return this.firstName + \u0026#39; \u0026#39; + this.lastName; // } } }) \u0026lt;/script\u0026gt; 应用场景: 购物车选中多个商品，计算总价格。\n另：计算属性只计算一次，有缓存，相比method计算速度更快。\n计算属性setter和getter 计算属性和methods的对比 ※ 计算属性内部实现缓存，如果值不改变，直接调用缓存中的内容，而methods每次都会调用，效率比计算属性差\n补充：ES6语法 let/var let有if和for块级作用域，而var没有if和for块级作用域。\nconst 标识符修饰为常量。定义时就需要赋值。对象不能修改，但是可以改变对象里面的属性。\n对象的字面量增强写法 属性的增强写法 函数的增强写法 事件监听 v-on的基本使用 v-on语法糖：@\nv-on的参数 事件监听的时候，若不需要传参数，则可以不写调用的小括号。（省略）\n如何给事件监听函数传入，游览器生成的event对象？\n传入的参数为$event就可以了。方法形参为event。\nv-on的修饰符 @click.stop防止事件冒泡。 @click.prevent阻止默认行为的执行。 @keyUp.enter监听enter键的释放。 @click.once只监听一次点击事件。 关于事件监听参考 Vue事件监听\n条件判断 v-if的使用 另：一般情况下，v-else-if、v-else不在HTML模板中使用，逻辑不放在界面代码处。\nv-show的使用 决定标签是否在界面显示：\u0026lt;h2 v-show=\u0026quot;isShow\u0026quot;\u0026gt; \u0026lt;/h2\u0026gt;\nv-if 和 v-show的区别。 （v-if经常使用）\nv-if：当条件为false时，包含v-if指令的元素，根本不会存在dom中。而v-show：当条件为false时，只是给元素添加了一个行内样式：display：none。前者dom中取掉，后者改样式。当展示切换频率很高，用v-show。\n循环遍历v-for的使用 遍历数组 ※ 1 \u0026lt;li v-for=\u0026#34;(item, index) in movies\u0026#34;\u0026gt;{{item}}\u0026lt;/li\u0026gt; 遍历对象 1 2 3 4 5 \u0026lt;!-- 获取到的是value --\u0026gt; \u0026lt;li v-for=\u0026#34;item in info\u0026#34;\u0026gt;{{item}}\u0026lt;/li\u0026gt; \u0026lt;!-- 获取到value和key --\u0026gt; \u0026lt;li v-for=\u0026#34;(value, key) in info\u0026#34;\u0026gt;{{value}} {{key}}\u0026lt;/li\u0026gt; v-for绑定key 省略。\n数组中响应式方法：数组内容改变，展示内容跟着改变。 push()、pop()、shift()、unshift()、splice()、sort()、reverse()\n另:\nsplice() ※ letters[index] = \u0026lsquo;change\u0026rsquo; 索引方式改变数组是非响应式的。 上面的方式可以改用为vue内部实现的函数vue.set()，更推荐splice()方式。 补充：三个高阶函数 ※ filter、reduce、map函数\nlet total = nums.filter(n =\u0026gt; n\u0026lt;100).map(n =\u0026gt; n * 2).reduce((pre, n) =\u0026gt; pre + n);\nv-model双向绑定 （暂时跳过） 三、组件化开发 ※ 什么是组件化？ 将复杂问题，拆分成很多个小问题。将一个页面拆分成一个个小的功能块，每个功能块完成属于自己这一部分独立的功能，那么整个页面的管理和维护就变得非常容易了。应用最后抽象为一个组件树。\n组件化思想 任何的应用都会被抽象成一颗组件树。 有了组件化的思想，我们在之后的开发中就要充分的利用它。 尽可能的将页面拆分成一个个小的、可复用的组件。 这样让我们的代码更加方便组织和管理，并且扩展性也更强。 组件使用步骤 创建组件构造器。 注册组件。 使用组件。 另：\n开发中用的最多的是局部组件。 一般只创建一个Vue实例。 全局组件和局部组件 在Vue实例创建之外，注册的组件为全局组件。\n1 2 // 注册全局组件(全局组件, 意味着可以在多个Vue的实例下面使用，app，app2) Vue.component(\u0026#39;cpn\u0026#39;, cpnC) 在Vue实例创建时，注册的组件为局部组件。 用的最多的还是局部组件。 ※ ※\n1 2 3 4 5 6 7 8 9 10 11 const app = new Vue({ el: \u0026#39;#app\u0026#39;, data: { message: \u0026#39;你好啊\u0026#39; }, // 局部组件 components: { // cpn为使用组件时的标签名，cpnC为组件实例 cpn: cpnC } }) 父组件和子组件的区别 ※ ※ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 1.创建第一个组件构造器(子组件)，Vue.extend()语法很少见了 const cpnC1 = Vue.extend({ template: ` \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;我是标题1\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;我是内容, 哈哈哈哈\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ` }) // 2.创建第二个组件构造器(父组件) const cpnC2 = Vue.extend({ template: ` \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;我是标题2\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;我是内容, 呵呵呵呵\u0026lt;/p\u0026gt; \u0026lt;cpn1\u0026gt;\u0026lt;/cpn1\u0026gt; \u0026lt;/div\u0026gt; `, components: { cpn1: cpnC1 } }) 注册组件的语法糖写法（推荐写法）※ ※ 省略 Vue.extend()，使用语法糖写法。 后面内容讲解，将tepmlate抽离出去，代码会更加清晰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Vue.component(\u0026#39;cpn1\u0026#39;, { template: ` \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;我是标题1\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;我是内容, 哈哈哈哈\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ` }) // 2.注册局部组件的语法糖 const app = new Vue({ el: \u0026#39;#app\u0026#39;, data: { message: \u0026#39;你好啊\u0026#39; }, components: { \u0026#39;cpn2\u0026#39;: { template: ` \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;我是标题2\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;我是内容, 呵呵呵\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ` } } }) 模板的分离写法 为什么组件data必须是函数？ ※ 1 2 3 4 5 6 7 8 9 10 // 1.注册一个全局组件 Vue.component(\u0026#39;cpn\u0026#39;, { template: \u0026#39;#cpn\u0026#39;, // 组件数据 data() { return { title: \u0026#39;abc\u0026#39; } } }) Vue组件应该有自己保存数据的地方，不能访问Vue实例中的数据。 组件对象也有一个data属性(函数)，也可以有methods等属性。 这个data属性必须是一个函数。 这个函数返回一个对象，对象内部保存着数据，多个相同组件各自使用各自的数据。 \u0026lt;script\u0026gt;标签 \u0026lt;template\u0026gt;标签 父子组件的通信 ※ ※ ※ 父组件向子组件 props (常用) ※ props : properties 属性\n通过props向子组件传递数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;!--\u0026lt;cpn v-bind:cmovies=\u0026#34;movies\u0026#34;\u0026gt;\u0026lt;/cpn\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;cpn cmovies=\u0026#34;movies\u0026#34; cmessage=\u0026#34;message\u0026#34;\u0026gt;\u0026lt;/cpn\u0026gt;--\u0026gt; \u0026lt;!-- v-bind --\u0026gt; \u0026lt;cpn :cmessage=\u0026#34;message\u0026#34; :cmovies=\u0026#34;movies\u0026#34;\u0026gt;\u0026lt;/cpn\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;template id=\u0026#34;cpn\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li v-for=\u0026#34;item in cmovies\u0026#34;\u0026gt;{{item}}\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;h2\u0026gt;{{cmessage}}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script src=\u0026#34;../js/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // 父传子: props const cpn = { template: \u0026#39;#cpn\u0026#39;, // props: [\u0026#39;cmovies\u0026#39;, \u0026#39;cmessage\u0026#39;], 这种有些怪，字符串居然是变量！ props: { // 1.类型限制 // cmovies: Array, // cmessage: String, // 2.提供一些默认值, 以及必传值 cmessage: { type: String, default: \u0026#39;aaaaaaaa\u0026#39;, required: true }, // 类型是对象或者数组时, 默认值必须是一个函数 js 中属性和函数的区别？？？ cmovies: { type: Array, default() { return [] } } }, data() { return {} }, methods: { } } const app = new Vue({ el: \u0026#39;#app\u0026#39;, data: { message: \u0026#39;你好啊\u0026#39;, movies: [\u0026#39;海王\u0026#39;, \u0026#39;海贼王\u0026#39;, \u0026#39;海尔兄弟\u0026#39;] }, components: { cpn // 对象字面量增强 } }) \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 问题：\nTODO\n1 \u0026lt;cpn :cmessage=\u0026#34;message\u0026#34; :cmovies=\u0026#34;movies\u0026#34;\u0026gt;\u0026lt;/cpn\u0026gt; 子组件向父组件 $emit events ※ 通过事件events向父组件发送消息。\n看到P61\nwebpack的介绍与安装 Node, Npm 和 webpack\n从本质上来讲，webpack是一个现代的JavRScript应用的静态模块打包工具。 webpack其中一个核心就是让我们可能进行模块化开发，并且会帮助我们处理模块间的依赖关系。 不仅仅是JavaScript文件，我们的CSS、图片、json文件等等在webpack中都可以被当做模块来使用（在后续我们会看到）。 认识webpack webpack的安装 webpack的起步 webpack的配置 loader的使用 Iwebpack中配置Vue plugin的使用 搭建本地服务器 四、Vue CLI详解 （脚手架）※ 五、vue-router 六、Vuex详解 （状态管理） 七、网络封装 八、项目实战 一步步学习，没什么耐心，那就项目驱动学习吧，开干。\n创建项目 Vue CLI教程Vue CLI 1 2 sudo cnpm install -g vue-cli # 安装 vue create supermall # 创新新的项目 目录结构划分 参考：https://blog.csdn.net/weixin_42290927/article/details/94432587\n九、项目部署 十、vuejs原理相关 ","date":"2021-01-06T22:02:52Z","permalink":"https://qiyueliuhuo.github.io/posts/vue%E5%AD%A6%E4%B9%A0/","title":"Vue学习"},{"content":"阅读开源代码是提高写代码能力最好的方式之一，现在入坑机器学习领域，在众多框架中选择pytorch框架作为自己基本的深度学习框架，现在开始从最简单的pytorch代码部分阅读，学习下python技巧顺便深入学习下神经网络的代码实践。\n关于阅读源码，我一开始在项目中调用pytorch模块和类，通过pycharm追踪源码，在pycharm中可以直接查看依赖包torch中源码，但是这样也不方便阅读torch源码，一个原因是pycharm中查看依赖包的源码内容，查看源码文件的方式是只读模式，没法在里面注释一些自己理解的东西，另一个原因是通过单步调试阅读代码是最容易理解代码的方式，想了想pytorch源码里面应该有对源码的测试代码可以直接跑起来，然后一步步运行，这样看着更方便。\n从github上下载pytorch代码，通过CONTRIBUTING.md文件中指导编译源码，注意使用：python setup.py develop 以开发模式构建代码。这个过程在我的电脑上要跑将近两小时。。。忘了什么原因导致我跑了两次。。。四个小时没了(呜呜\u0026hellip;) 先从加载和处理数据源码了解。\n注：笔者阅读的pytorch版本为1.7.0，torchvision版本为0.6\nDataset 相关源码 MNIST类 类继承图：\n先来看看最常用的mnist数据集的使用，下面两行使用torchvision中提供的MNIST数据集类定义了两个数据集类，分别为训练数据集和测试数据集，那么为什么需要定义两个同样的数据集呢？其中一个因为是训练和测试使用的数据集不一样；另一个原因是训练和测试时候对应的batch_size大小不一样。下面进一步分析datasets.MNIST代码。\n1 2 3 4 5 6 dataset1 = datasets.MNIST(\u0026#39;../data\u0026#39;, train=True, download=True, transform=transform) dataset2 = datasets.MNIST(\u0026#39;../data\u0026#39;, train=False, transform=transform) train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs) test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs) 下面的代码来自torchvision.datasets.mnist模块\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from .vision import VisionDataset import warnings from PIL import Image import os import os.path import numpy as np import torch import codecs import string from .utils import download_url, download_and_extract_archive, extract_archive, \\ verify_str_arg class MNIST(VisionDataset): resources = [ (\u0026#34;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u0026#34;, \u0026#34;f68b3c2dcbeaaa9fbdd348bbdeb94873\u0026#34;), (\u0026#34;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u0026#34;, \u0026#34;d53e105ee54ea40749a09fcbcd1e9432\u0026#34;), (\u0026#34;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u0026#34;, \u0026#34;9fb629c4189551a2d022fa330f9573f3\u0026#34;), (\u0026#34;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u0026#34;, \u0026#34;ec29112dd5afa0611ce80d1b7f02629c\u0026#34;) ] training_file = \u0026#39;training.pt\u0026#39; test_file = \u0026#39;test.pt\u0026#39; classes = [\u0026#39;0 - zero\u0026#39;, \u0026#39;1 - one\u0026#39;, \u0026#39;2 - two\u0026#39;, \u0026#39;3 - three\u0026#39;, \u0026#39;4 - four\u0026#39;, \u0026#39;5 - five\u0026#39;, \u0026#39;6 - six\u0026#39;, \u0026#39;7 - seven\u0026#39;, \u0026#39;8 - eight\u0026#39;, \u0026#39;9 - nine\u0026#39;] @property def train_labels(self): warnings.warn(\u0026#34;train_labels has been renamed targets\u0026#34;) return self.targets @property def test_labels(self): warnings.warn(\u0026#34;test_labels has been renamed targets\u0026#34;) return self.targets @property def train_data(self): warnings.warn(\u0026#34;train_data has been renamed data\u0026#34;) return self.data @property def test_data(self): warnings.warn(\u0026#34;test_data has been renamed data\u0026#34;) return self.data 类MNIST继承自VisionDataset，从名称可以知道VisionDataset是对流行的视觉数据集的抽象类，我从子类开始分析，后面再阅读分析VisionDataset类。MNIST类前面定义了四个属性，其中training_file、test_file保存将原始Minst数据处理之后的tensor格式的训练和测试数据的文件名。另外有四个方法分别是train_labels、test_labels、train_data、test_data，它们都被@property注解所修饰，@property这种注解装饰器来创建只读属性，@property装饰器会将方法转换为相同名称的只读属性，可以与所定义的属性配合使用，这样可以防止属性被修改。那么我们就可以通过mnist.train_labels得到实例的私有属性，而实际上该过程调用的是train_labels()函数。上面部分除了@property其他的都很好理解了。下面来看这个类的其余部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__(self, root, train=True, transform=None, target_transform=None, download=False): super(MNIST, self).__init__(root, transform=transform, target_transform=target_transform) self.train = train # training set or test set if download: self.download() if not self._check_exists(): raise RuntimeError(\u0026#39;Dataset not found.\u0026#39; + \u0026#39; You can use download=True to download it\u0026#39;) if self.train: data_file = self.training_file else: data_file = self.test_file self.data, self.targets = torch.load(os.path.join(self.processed_folder, data_file)) def __getitem__(self, index): \u0026#34;\u0026#34;\u0026#34; Args: index (int): Index Returns: tuple: (image, target) where target is index of the target class. \u0026#34;\u0026#34;\u0026#34; img, target = self.data[index], int(self.targets[index]) # doing this so that it is consistent with all other datasets # to return a PIL Image img = Image.fromarray(img.numpy(), mode=\u0026#39;L\u0026#39;) # @3 if self.transform is not None: img = self.transform(img) # @1 if self.target_transform is not None: target = self.target_transform(target) # @2 return img, target def __len__(self): return len(self.data) __init__该类的构造器(constructor)函数入参transform和target_transform分别传入对数据图像和标签的预处理，在@1和@2处可以看到在每一张图像返回前分别对图像和标签进行用户自定义预处理。传入的train参数来决定该MNIST的dataset会返回训练集or测试集中的数据。\n__getitem__根据python3中定义，如果在类中定义了__getitem__()方法，那么它的实例对象（假设为P）就可以以P[key]形式取值，当实例对象做P[key]运算时，就会调用类中的__getitem__()方法，这是一个很有意思也很有用的特性。具体到这个类的key为训练样本的idex为int类型代表第几个数据样本。在**@3**处将图像转成PIL图像对象，该PIL对象最终会通过transform预处理为tonsor对象。类MNIST剩余部分最主要就是下载download函数，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @property def raw_folder(self): return os.path.join(self.root, self.__class__.__name__, \u0026#39;raw\u0026#39;) @property def processed_folder(self): return os.path.join(self.root, self.__class__.__name__, \u0026#39;processed\u0026#39;) @property def class_to_idx(self): return {_class: i for i, _class in enumerate(self.classes)} def _check_exists(self): return (os.path.exists(os.path.join(self.processed_folder, self.training_file)) and os.path.exists(os.path.join(self.processed_folder, self.test_file))) def download(self): \u0026#34;\u0026#34;\u0026#34;Download the MNIST data if it doesn\u0026#39;t exist in processed_folder already.\u0026#34;\u0026#34;\u0026#34; if self._check_exists(): # @4 return os.makedirs(self.raw_folder, exist_ok=True) os.makedirs(self.processed_folder, exist_ok=True) # download files for url, md5 in self.resources: filename = url.rpartition(\u0026#39;/\u0026#39;)[2] download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5) # process and save as torch files print(\u0026#39;Processing...\u0026#39;) training_set = ( read_image_file(os.path.join(self.raw_folder, \u0026#39;train-images-idx3-ubyte\u0026#39;)), read_label_file(os.path.join(self.raw_folder, \u0026#39;train-labels-idx1-ubyte\u0026#39;)) ) test_set = ( read_image_file(os.path.join(self.raw_folder, \u0026#39;t10k-images-idx3-ubyte\u0026#39;)), read_label_file(os.path.join(self.raw_folder, \u0026#39;t10k-labels-idx1-ubyte\u0026#39;)) ) with open(os.path.join(self.processed_folder, self.training_file), \u0026#39;wb\u0026#39;) as f: torch.save(training_set, f) with open(os.path.join(self.processed_folder, self.test_file), \u0026#39;wb\u0026#39;) as f: torch.save(test_set, f) print(\u0026#39;Done!\u0026#39;) def extra_repr(self): return \u0026#34;Split: {}\u0026#34;.format(\u0026#34;Train\u0026#34; if self.train is True else \u0026#34;Test\u0026#34;) download函数下载原始mnist数据并将训练图像数据、训练标签数据存储在一个training_file文件中，将测试图像数据、测试标签数据存在test_file文件中，@4处可以看到当processed_folder目录下存在这两个文件，那么就不会通过网络下载，直接读取本地训练测试数据文件。另外，像download_and_extract_archive函数的具体实现，并没有在上面给出，这个就是更加细节的代码实现，此次代码阅读会忽略很多具体细节实现，专注于大的框架实现。\nMNIST类继承自VisionDataset类，下面我们看看这个类中抽象出了哪些功能。\nVisionDataset类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class VisionDataset(data.Dataset): _repr_indent = 4 def __init__(self, root, transforms=None, transform=None, target_transform=None): if isinstance(root, torch._six.string_classes): root = os.path.expanduser(root) self.root = root has_transforms = transforms is not None has_separate_transform = transform is not None or target_transform is not None if has_transforms and has_separate_transform: raise ValueError(\u0026#34;Only transforms or transform/target_transform can \u0026#34; \u0026#34;be passed as argument\u0026#34;) # for backwards-compatibility @5 self.transform = transform self.target_transform = target_transform if has_separate_transform: transforms = StandardTransform(transform, target_transform) # @6 self.transforms = transforms def __getitem__(self, index): raise NotImplementedError def __len__(self): raise NotImplementedError def __repr__(self): head = \u0026#34;Dataset \u0026#34; + self.__class__.__name__ body = [\u0026#34;Number of datapoints: {}\u0026#34;.format(self.__len__())] if self.root is not None: body.append(\u0026#34;Root location: {}\u0026#34;.format(self.root)) body += self.extra_repr().splitlines() if hasattr(self, \u0026#34;transforms\u0026#34;) and self.transforms is not None: body += [repr(self.transforms)] lines = [head] + [\u0026#34; \u0026#34; * self._repr_indent + line for line in body] return \u0026#39;\\n\u0026#39;.join(lines) def _format_transform_repr(self, transform, head): lines = transform.__repr__().splitlines() return ([\u0026#34;{}{}\u0026#34;.format(head, lines[0])] + [\u0026#34;{}{}\u0026#34;.format(\u0026#34; \u0026#34; * len(head), line) for line in lines[1:]]) def extra_repr(self): return \u0026#34;\u0026#34; VisionDataset类中在**@5处注释向后兼容，可以看到在构造器（constructor）的入参有transforms、transform、target_transform三个预处理变量，那么哪一种是方式是重构后新引入的预处理方式呢？其实从@6**处就可以知道，为了兼容transform、target_transform传入方式，将它们两个构造为StandardTransform，也就是说重构后，希望用transforms(注意有个s)入参取代原有的方式。并且，机智的我（嘿嘿）通过查看该文件的修改历史，如下图，就可以看到源码作者的重构的意图。\n再来看看VisionDataset类中__repr__()的作用，简单理解就是，python中定义当输出print(instance)时，等同于执行print(instance.__repr__())，这样用户就可以通过实现自己的__repr__函数来控制我们想要的信息。默认情况下，__repr__() 会返回和调用者有关的 “类名+object at+内存地址”信息。另外，若调用repr(instance)也会执行__repr__()函数。\nVisionDataset类继承自data.Dataset类，下面我们看看这个类中有哪些属性和方法。\nDataset torch.utils.data.dataset模块中定义了几个数据类，是对pytorch搭建神经网络训练和测试所需要的数据集的封装和抽象。其中Dataset类是所有数据集类的父类，那么来看看这个类中定义了什么内容吧~\n下面代码是我从pytorch源代码中拷贝过来的，省略了一些模块和类的导入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 T_co = TypeVar(\u0026#39;T_co\u0026#39;, covariant=True) T = TypeVar(\u0026#39;T\u0026#39;) class Dataset(Generic[T_co]): r\u0026#34;\u0026#34;\u0026#34;An abstract class representing a :class:`Dataset`. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:`__len__`, which is expected to return the size of the dataset by many :class:`~torch.utils.data.Sampler` implementations and the default options of :class:`~torch.utils.data.DataLoader`. .. note:: :class:`~torch.utils.data.DataLoader` by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided. \u0026#34;\u0026#34;\u0026#34; def __getitem__(self, index) -\u0026gt; T_co: raise NotImplementedError def __add__(self, other: \u0026#39;Dataset[T_co]\u0026#39;) -\u0026gt; \u0026#39;ConcatDataset[T_co]\u0026#39;: return ConcatDataset([self, other]) 前两行定义两个用户自定义泛型，这种泛型编程自python3.5起开始引入到python中，有点类似于c++中泛型编程（不怎么用，忘得差不多了），c++中泛型会在编译器做类型检查和替换，属于强制类型检查，而在python中因为python类型都是弱类型，所以这种泛型编程更多的是给静态类型检测工具提供说明，帮助我们在代码编写阶段正确使用和传递python变量。目前，先忽略这些泛型类型。\n在Dataset类中定义了两个基本函数，__getitem__实现key-value结构，__add__定义两个数据集叠加的操作。\n下来我们来看看torchvision中其他的dataset。\n类继承图：\nImageFolder 下面源码摘抄自torchvision.datasets.folder模块。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class ImageFolder(DatasetFolder): \u0026#34;\u0026#34;\u0026#34;A generic data loader where the images are arranged in this way: :: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. is_valid_file (callable, optional): A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files) Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples \u0026#34;\u0026#34;\u0026#34; def __init__(self, root, transform=None, target_transform=None, loader=default_loader, is_valid_file=None): super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None, transform=transform, target_transform=target_transform, is_valid_file=is_valid_file) self.imgs = self.samples ImageFolder是一个通用的图像数据集类，它要求数据按照：\nroot/label01/xxx.png\nroot/label01/xxy.png\nroot/label01/xxz.png\nroot/label02/123.png\nroot/label02/nsdf3.png\nroot/label02/asd932_.png\n的格式存放。这是分类数据集的数据和标签表示的另一种方式，通过目录名当作标签来存放图像数据，对于制作自定义数据集还是挺方便的。\nImageFolder继承自DatasetFolder，从名称就可以知道ImageFolder是类DatasetFolder的具体化，它只用来处理图像数据集，一般处理IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp') 中的图像格式。ImageFolder读取数据集后，得到三个属性：classes、class_to_idx、imgs。\nSampler 相关源码 源码位于 torch/utils/data/sampler.py，那么为什么要有Sampler相关类呢？我觉得可以这样理解：Dataset是为数据的总体，每次训练或者测试要从总体中随机或顺序抽取一个样本或者一批样本，那么不同的Sampler子类就表示从数据集中不同的抽取方式。\nSampler：所有Sampler的父类。 SequentialSampler：顺序依次获取下标。 RandomSampler：乱序获取下标。 SubsetRandomSampler：某个子集内乱序获取下标。 WeightedRandomSampler：为每个样本设置权重，权重大表示获取概率高。 BatchSampler：即将若干个样本形成一个batch。 Sampler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class Sampler(Generic[T_co]): r\u0026#34;\u0026#34;\u0026#34;Base class for all Samplers. Every Sampler subclass has to provide an :meth:`__iter__` method, providing a way to iterate over indices of dataset elements, and a :meth:`__len__` method that returns the length of the returned iterators. .. note:: The :meth:`__len__` method isn\u0026#39;t strictly required by :class:`~torch.utils.data.DataLoader`, but is expected in any calculation involving the length of a :class:`~torch.utils.data.DataLoader`. \u0026#34;\u0026#34;\u0026#34; def __init__(self, data_source: Optional[Sized]) -\u0026gt; None: pass def __iter__(self) -\u0026gt; Iterator[T_co]: raise NotImplementedError # NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ] # # Many times we have an abstract class representing a collection/iterable of # data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally # implementing a `__len__` method. In such cases, we must make sure to not # provide a default implementation, because both straightforward default # implementations have their issues: # # + `return NotImplemented`: # Calling `len(subclass_instance)` raises: # TypeError: \u0026#39;NotImplementedType\u0026#39; object cannot be interpreted as an integer # # + `raise NotImplementedError()`: # This prevents triggering some fallback behavior. E.g., the built-in # `list(X)` tries to call `len(X)` first, and executes a different code # path if the method is not found or `NotImplemented` is returned, while # raising an `NotImplementedError` will propagate and and make the call # fail where it could have use `__iter__` to complete the call. # # Thus, the only two sensible things to do are # # + **not** provide a default `__len__`. # # + raise a `TypeError` instead, which is what Python uses when users call # a method that is not defined on an object. # (@ssnl verifies that this works on at least Python 3.7.) Sampler类是所有Sampler的父类，类中__init__该类的构造器(constructor)函数，__iter__提供迭代数据集元素索引的方法。\nDataLoader相关源码 源码位于pytorch源码目录的torch/utils/data/dataloader.py，DataLoader类非常重要，它是我们作为深度学习模型训练与开发过程中直接用到的类，也就是说我们一般情况下，通过它来将我们的数据集数据加载到程序中，通常是用一个for循环遍历它，DataLoader类依赖Dataset类和Sampler类在内部为我们实现了很多数据集遍历的方式，方便于多种场景下使用。\n分析DataLoader类之前，我先在之前学习pytorch的学习材料中，摘抄了一部分使用DataLoader的代码，先看看具体如何使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 trainset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) # ... 省略部分代码 # 训练网络 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(\u0026#39;[%d, %5d] loss: %.3f\u0026#39; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print(\u0026#39;Finished Training\u0026#39;) 上面代码是一个简单的训练过程，我们可以看到使用Dataset和DataLoader很方便地为模型训练提供数据加载、打乱、预处理、甚至是多进程加载。作为pytorch的使用者，准确详细的了解pytorch各种包，可以熟练快速开发我们自己的训练程序，下面先具体分析下DataLoader的接口。\n源码比较长，我分析这样比较长的源码，将比较简单部分的代码解释直接写在源码里，作为注释；将复杂部分通过@符号标注，然后在源码后面部分分析。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class DataLoader(Generic[T_co]): r\u0026#34;\u0026#34;\u0026#34; 原注释省略。 \u0026#34;\u0026#34;\u0026#34; dataset: Dataset[T_co] batch_size: Optional[int] num_workers: int pin_memory: bool drop_last: bool timeout: float sampler: Sampler prefetch_factor: int _iterator : Optional[\u0026#39;_BaseDataLoaderIter\u0026#39;] __initialized = False def __init__(self, dataset: Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False): torch._C._log_api_usage_once(\u0026#34;python.data_loader\u0026#34;) # type: ignore if num_workers \u0026lt; 0: raise ValueError(\u0026#39;num_workers option should be non-negative; \u0026#39; \u0026#39;use num_workers=0 to disable multiprocessing.\u0026#39;) if timeout \u0026lt; 0: raise ValueError(\u0026#39;timeout option should be non-negative\u0026#39;) if num_workers == 0 and prefetch_factor != 2: raise ValueError(\u0026#39;prefetch_factor option could only be specified in multiprocessing.\u0026#39; \u0026#39;let num_workers \u0026gt; 0 to enable multiprocessing.\u0026#39;) assert prefetch_factor \u0026gt; 0 if persistent_workers and num_workers == 0: raise ValueError(\u0026#39;persistent_workers option needs num_workers \u0026gt; 0\u0026#39;) self.dataset = dataset self.num_workers = num_workers self.prefetch_factor = prefetch_factor self.pin_memory = pin_memory self.timeout = timeout self.worker_init_fn = worker_init_fn self.multiprocessing_context = multiprocessing_context # Arg-check dataset related before checking samplers because we want to # tell users that iterable-style datasets are incompatible with custom # samplers first, so that they don\u0026#39;t learn that this combo doesn\u0026#39;t work # after spending time fixing the custom sampler errors. if isinstance(dataset, IterableDataset): self._dataset_kind = _DatasetKind.Iterable # NOTE [ Custom Samplers and IterableDataset ] # # `IterableDataset` does not support custom `batch_sampler` or # `sampler` since the key is irrelevant (unless we support # generator-style dataset one day...). # # For `sampler`, we always create a dummy sampler. This is an # infinite sampler even when the dataset may have an implemented # finite `__len__` because in multi-process data loading, naive # settings will return duplicated data (which may be desired), and # thus using a sampler with length matching that of dataset will # cause data lost (you may have duplicates of the first couple # batches, but never see anything afterwards). Therefore, # `Iterabledataset` always uses an infinite sampler, an instance of # `_InfiniteConstantSampler` defined above. # # A custom `batch_sampler` essentially only controls the batch size. # However, it is unclear how useful it would be since an iterable-style # dataset can handle that within itself. Moreover, it is pointless # in multi-process data loading as the assignment order of batches # to workers is an implementation detail so users can not control # how to batchify each worker\u0026#39;s iterable. Thus, we disable this # option. If this turns out to be useful in future, we can re-enable # this, and support custom samplers that specify the assignments to # specific workers. if shuffle is not False: raise ValueError( \u0026#34;DataLoader with IterableDataset: expected unspecified \u0026#34; \u0026#34;shuffle option, but got shuffle={}\u0026#34;.format(shuffle)) elif sampler is not None: # See NOTE [ Custom Samplers and IterableDataset ] raise ValueError( \u0026#34;DataLoader with IterableDataset: expected unspecified \u0026#34; \u0026#34;sampler option, but got sampler={}\u0026#34;.format(sampler)) elif batch_sampler is not None: # See NOTE [ Custom Samplers and IterableDataset ] raise ValueError( \u0026#34;DataLoader with IterableDataset: expected unspecified \u0026#34; \u0026#34;batch_sampler option, but got batch_sampler={}\u0026#34;.format(batch_sampler)) else: self._dataset_kind = _DatasetKind.Map if sampler is not None and shuffle: raise ValueError(\u0026#39;sampler option is mutually exclusive with \u0026#39; \u0026#39;shuffle\u0026#39;) if batch_sampler is not None: # auto_collation with custom batch_sampler if batch_size != 1 or shuffle or sampler is not None or drop_last: raise ValueError(\u0026#39;batch_sampler option is mutually exclusive \u0026#39; \u0026#39;with batch_size, shuffle, sampler, and \u0026#39; \u0026#39;drop_last\u0026#39;) batch_size = None drop_last = False elif batch_size is None: # no auto_collation if drop_last: raise ValueError(\u0026#39;batch_size=None option disables auto-batching \u0026#39; \u0026#39;and is mutually exclusive with drop_last\u0026#39;) if sampler is None: # give default samplers if self._dataset_kind == _DatasetKind.Iterable: # See NOTE [ Custom Samplers and IterableDataset ] sampler = _InfiniteConstantSampler() else: # map-style if shuffle: # Cannot statically verify that dataset is Sized # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ] sampler = RandomSampler(dataset, generator=generator) # type: ignore else: sampler = SequentialSampler(dataset) if batch_size is not None and batch_sampler is None: # auto_collation without custom batch_sampler batch_sampler = BatchSampler(sampler, batch_size, drop_last) self.batch_size = batch_size self.drop_last = drop_last self.sampler = sampler self.batch_sampler = batch_sampler self.generator = generator if collate_fn is None: if self._auto_collation: collate_fn = _utils.collate.default_collate else: collate_fn = _utils.collate.default_convert self.collate_fn = collate_fn self.persistent_workers = persistent_workers self.__initialized = True self._IterableDataset_len_called = None # See NOTE [ IterableDataset and __len__ ] self._iterator = None def _get_iterator(self) -\u0026gt; \u0026#39;_BaseDataLoaderIter\u0026#39;: if self.num_workers == 0: return _SingleProcessDataLoaderIter(self) else: return _MultiProcessingDataLoaderIter(self) @property def multiprocessing_context(self): return self.__multiprocessing_context @multiprocessing_context.setter def multiprocessing_context(self, multiprocessing_context): if multiprocessing_context is not None: if self.num_workers \u0026gt; 0: if not multiprocessing._supports_context: raise ValueError(\u0026#39;multiprocessing_context relies on Python \u0026gt;= 3.4, with \u0026#39; \u0026#39;support for different start methods\u0026#39;) if isinstance(multiprocessing_context, string_classes): valid_start_methods = multiprocessing.get_all_start_methods() if multiprocessing_context not in valid_start_methods: raise ValueError( (\u0026#39;multiprocessing_context option \u0026#39; \u0026#39;should specify a valid start method in {!r}, but got \u0026#39; \u0026#39;multiprocessing_context={!r}\u0026#39;).format(valid_start_methods, multiprocessing_context)) # error: Argument 1 to \u0026#34;get_context\u0026#34; has incompatible type \u0026#34;Union[str, bytes]\u0026#34;; expected \u0026#34;str\u0026#34; [arg-type] multiprocessing_context = multiprocessing.get_context(multiprocessing_context) # type: ignore if not isinstance(multiprocessing_context, python_multiprocessing.context.BaseContext): raise TypeError((\u0026#39;multiprocessing_context option should be a valid context \u0026#39; \u0026#39;object or a string specifying the start method, but got \u0026#39; \u0026#39;multiprocessing_context={}\u0026#39;).format(multiprocessing_context)) else: raise ValueError((\u0026#39;multiprocessing_context can only be used with \u0026#39; \u0026#39;multi-process loading (num_workers \u0026gt; 0), but got \u0026#39; \u0026#39;num_workers={}\u0026#39;).format(self.num_workers)) self.__multiprocessing_context = multiprocessing_context def __setattr__(self, attr, val): if self.__initialized and attr in ( \u0026#39;batch_size\u0026#39;, \u0026#39;batch_sampler\u0026#39;, \u0026#39;sampler\u0026#39;, \u0026#39;drop_last\u0026#39;, \u0026#39;dataset\u0026#39;, \u0026#39;persistent_workers\u0026#39;): raise ValueError(\u0026#39;{} attribute should not be set after {} is \u0026#39; \u0026#39;initialized\u0026#39;.format(attr, self.__class__.__name__)) super(DataLoader, self).__setattr__(attr, val) # We quote \u0026#39;_BaseDataLoaderIter\u0026#39; since it isn\u0026#39;t defined yet and the definition can\u0026#39;t be moved up # since \u0026#39;_BaseDataLoaderIter\u0026#39; references \u0026#39;DataLoader\u0026#39;. def __iter__(self) -\u0026gt; \u0026#39;_BaseDataLoaderIter\u0026#39;: # When using a single worker the returned iterator should be # created everytime to avoid reseting its state # However, in the case of a multiple workers iterator # the iterator is only created once in the lifetime of the # DataLoader object so that workers can be reused if self.persistent_workers and self.num_workers \u0026gt; 0: if self._iterator is None: self._iterator = self._get_iterator() else: self._iterator._reset(self) return self._iterator else: return self._get_iterator() @property def _auto_collation(self): return self.batch_sampler is not None @property def _index_sampler(self): # The actual sampler used for generating indices for `_DatasetFetcher` # (see _utils/fetch.py) to read data at each time. This would be # `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise. # We can\u0026#39;t change `.sampler` and `.batch_sampler` attributes for BC # reasons. if self._auto_collation: return self.batch_sampler else: return self.sampler def __len__(self) -\u0026gt; int: if self._dataset_kind == _DatasetKind.Iterable: # NOTE [ IterableDataset and __len__ ] # # For `IterableDataset`, `__len__` could be inaccurate when one naively # does multi-processing data loading, since the samples will be duplicated. # However, no real use case should be actually using that behavior, so # it should count as a user error. We should generally trust user # code to do the proper thing (e.g., configure each replica differently # in `__iter__`), and give us the correct `__len__` if they choose to # implement it (this will still throw if the dataset does not implement # a `__len__`). # # To provide a further warning, we track if `__len__` was called on the # `DataLoader`, save the returned value in `self._len_called`, and warn # if the iterator ends up yielding more than this number of samples. # Cannot statically verify that dataset is Sized length = self._IterableDataset_len_called = len(self.dataset) # type: ignore if self.batch_size is not None: # IterableDataset doesn\u0026#39;t allow custom sampler or batch_sampler from math import ceil if self.drop_last: length = length // self.batch_size else: length = ceil(length / self.batch_size) return length else: return len(self._index_sampler) ","date":"2020-12-05T23:21:34Z","permalink":"https://qiyueliuhuo.github.io/posts/pytorch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E4%B8%80/","title":"Pytorch源码阅读(一)：Dataset和DataLoader"},{"content":"\n前言 支持向量机即Support Vector Machine，简称SVM。我最开始听说这个机器的时候，一头雾水，“支持”，“向量”，“机”，这三个词组合在一起让我在一开始觉得这个东西好抽象。首先，SVM实际上是一个二分类模型，也可以用来解决回归问题。了解过一些原理后，我觉得这种模型（方法）被称为Vector Support Machine更好一些，简单且不严谨的理解是: 这种模型通过某些向量（高维空间中的点）支持（支撑）而形成的一种机器（方法），或者称之为模型。\n引用Free Mind在介绍SVM中一句话：“SVM 一直被认为是效果最好的现成可用的分类算法之一（其实有很多人都相信，“之一”是可以去掉的）”。刚开始接触机器学习（大半年🤦‍♂️），目前没有做太多实践，并不清楚SVM的效果有多好，后面应该多了解些SVM的实际应用。\n支持向量机原理 现在有一些数据$\\mathbf{x}={\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\boldsymbol{x_3}, \u0026hellip;, \\boldsymbol{x_n}}$，这些数据的标签也已知$\\mathbf{y}={y_1, y_2, y_3, \u0026hellip;, y_n}$。其中每个数据$\\boldsymbol{x_i}$为有两个特征值，即$\\boldsymbol{x_i} = (x_{i1}, x_{i2})$，为二维空间上的一点，标签$y_i\\in{-1, +1}$，一般我们将$+1$称作正类，将$-1$称为负类。这里假设$\\boldsymbol{x_i}$只有两个特征值是为了能够在二维坐标中将数据表示出来，方便理解。更一般地，$\\boldsymbol{x}_{i} \\in \\mathbb{R}^{n}$，即每个数据有$n$个特征值，每个数据都是$n$维空间中的一个点。\n下图在二维坐标系中将这些数据表示出来，其中红色代表正类，蓝色代表负类。 可能有人要问了：为什么这些点分布的很有规律（特点）？即正类的这些红色点都分布在一边，而蓝色的这些数据点分布在另外一边。的确，现实中的数据往往不会这么理想，可能会是两部分数据交叉在一起，并不是直观上那么明显的分为两边，这里我们从最简单的原数据线性可分的问题入手，所以先假设数据比较理想。之后我们会慢慢介绍一些复杂数据的情况是如何处理的。\n那么如果有上图一些较为理想的数据，需要寻找到一个线性边界将它们划分开来，如何找到这个线性边界呢？这个问题大概小学生也能做到。如下图，我们在红色数据点和蓝色数据点之间画一条直线，使得两类点分割到直线的两边，这条直线显然就是一个线性边界，并且可以正确地将所给的数据集分为两类。 然而，或许你想说你画出来的直线和上图不一样，它或许是下图中的某一条。 这里，我们很容易想到这样的直线有无数条。用这些直线对于已知的上图数据分类，它们的确都可以产生正确的分类结果。然而，机器学习建模的目的并不是对于已知点（标签已知）成功分类即可，学习的目的是需要对于未知情况也能够适用，即对于一个未知分类的数据点，我们的建立的模型需要告诉我们这个数据点它是属于哪一类的，正类或者是负类。机器学习建立的模型试图去寻找数据中一般的规律，而不是对于已有数据正确的表示。\n假如对于一个新的未知分类的数据，它在二维坐标位置为X所在的位置如下图。 这里，我们用上面得到的这些直线对新的数据X分类，$l_1$直线和其他直线分类的结果并不一样，它处在$l_1$右侧，将它标记为负类；它处在其他直线的左侧，将它标记为正类。不同的线性模型对于新的数据的分类结果不一样，那么，如何衡量这些直线的好坏，哪一个直线最好，更能代表这些数据中的一般规律呢？\nSVM给出的答案是：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。 这也是SVM学习的基本想法。\n这里的几何间隔最大含义是：在无数正确划分数据集的分离超平面中，找到这样的超平面，该超平面离最近的数据点的距离最大。用数学语言描述为下面的约束最优化问题： $$ \\begin{array}{ll} \\max \\limits_{\\boldsymbol{w}, \\boldsymbol{b}} \u0026amp; \\gamma \\ \\text { s.t. } \u0026amp; y_{i}\\left(\\frac{\\boldsymbol{w}}{|\\boldsymbol{w}|} \\cdot \\boldsymbol{x_{i}}+\\frac{b}{|\\boldsymbol{w}|}\\right) \\geqslant \\gamma, \\quad i=1,2, \\cdots, N \\end{array} $$ 其中，第一行代表优化问题中目标函数，第二行代表约束条件，$s.t.$指的是subject to ，受限于。\n要理解上面的式子，让我们先来回顾下超平面的公式以及点到超平面的距离公式，可以参考这篇文章的推导超平面公式及点到超平面距离，超平面的公式为： $$ \\boldsymbol{w}^{T} \\boldsymbol{x}+b=0 $$ 其中，$w$是超平面的法向量，此处为列向量，简单的理解为：$w^{T} x$为法向量和超平面中的点向量（原点到超平面点形成的向量）的内积，该内积代表点向量在法向量上投影的距离；如果对超空间中的自由点施加一个法向量投影距离为$-b$时的拘束条件，那么所有满足约束条件的所有点处于超平面内。\n超空间中，点到超平面的距离公式为： $$ d=\\frac{|\\boldsymbol{w}^{T} \\boldsymbol{x}+b|}{|\\boldsymbol{w}|} $$ 其中$|\\boldsymbol{w}|$代表$\\boldsymbol{w}$向量的第二范数，也称欧几里得范数（距离）；其严格表示应该为$|\\boldsymbol{w}|_2$，因为比较常用常常省略右下角标2。\n让我们回到上面的SVM约束最优化问题： $$ \\begin{array}{ll} \\max \\limits_{\\boldsymbol{w}, \\boldsymbol{b}} \u0026amp; \\gamma \u0026amp;\u0026amp; (1) \\ \\text { s.t. } \u0026amp; y_{i}\\left(\\frac{\\boldsymbol{w}}{|\\boldsymbol{w}|} \\cdot \\boldsymbol{x_{i}}+\\frac{b}{|\\boldsymbol{w}|}\\right) \\geqslant \\gamma, \\quad i=1,2, \\cdots, N \u0026amp;\u0026amp; (2) \\end{array} $$ $\\gamma$代表的是数据点到超平面的几何距离，$(2)$式括号内代表的是每个点到超平面的几何距离，然而，不同于上面的距离公式，该公式没有加绝对值，括号部分的值可正可负，显然，正负对应于超平面的两侧，一开始我们有约定标签$y_i\\in{-1, +1}$，那么当我们约定括号部分值为负的那一侧代表负类，即$y_i=-1$；为正的那一侧代表正类，即$y_i=+1$，这里更多的是一种约定的问题，这种约定方便我们建模求解问题。\n我们可以试着理解下$(1) (2)$两式，当有某些数据点被分错时，$(2)$左侧结果为负值，那么$\\gamma$也为负值，此时$\\gamma$肯定不是最大值，只有当所有的数据点都被正确分类时（$\\gamma$为正），而且超平面距离最近的点的距离最远时，$\\gamma$达到最大。也就是说，如果我们的求解方法是对参数逐步迭代达到$\\gamma$最大时，那么一开始是由误分类点驱动，之后根据几何间隔最大驱动参数更新。 上面最优化建模完美得到了正确划分训练数据集并且几何间隔最大的目的。\nSVM约束最优化问题：\n$$ \\begin{array}{ll} \\max \\limits_{w, b} \u0026amp; \\gamma \u0026amp; {(1)} \\\n\\text { s.t. } \u0026amp; y_{i}\\left(\\frac{w}{|w|} \\cdot x_{i}+\\frac{b}{|w|}\\right) \\geqslant \\gamma, \\quad i=1,2, \\cdots, N \u0026amp; {(2)} \\end{array} $$\n后续讨论：\n数据并不是线性可分的。 数据存在误差，软间隔。 SVM优缺点 优点 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题； 能找出对任务至关重要的关键样本（即：支持向量）； 采用核技巧之后，可以处理非线性分类/回归任务； 最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。 缺点 训练时间长。当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为$O\\left(N^{2}\\right)$，其中 N 为训练样本的数量； 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O\\left(N^{2}\\right)$； 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。 目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。 SVM使用场景 现有的一些支持向量机软件包 LIBSVM \u0026ndash; A Library for Support Vector Machines\n一些废话 参考：\nsupport-vector-machines\n理解SVM的三层境界\n(https://zhuanlan.zhihu.com/p/77750026)\n","date":"2020-07-14T17:46:32Z","permalink":"https://qiyueliuhuo.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","title":"机器学习之支持向量机"},{"content":"前言 人工神经网络（Artificial Neural Network，ANN）是一门重要的机器学习技术。它是目前最为火热的研究方向\u0026ndash;深度学习（Deep Learning）的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，同时也可以更好地帮助你理解深度学习技术。\n四年前，韩国围棋九段棋手李世石与谷歌人工智能 AlphaGo 之间的围棋比赛，最终 AlphaGo 以四比一取胜，这让人工智能 AlphaGo 背后的技术——深度学习算法大火，笔者也是从这个时候开始关注深度学习，对计算机领域产生了兴趣。此后一年，谷歌又推出 AlphaGo 围棋升级版，并邀请世界围棋冠军、中国棋手柯洁于 2017 年 5 月份与之进行围棋大战，笔者全程直播观看了 AlphaGo 与柯洁人机围棋大战，在看到作为人类智慧的顶峰柯洁含泪认输后，笔者被 AlphaGo 背后的技术神经网络所深深震撼（还包括另一种技术：蒙特卡洛树搜索）。围棋不同于其他棋，计算机目前无法遍历围棋的所有可能的下法，所以算法必须尝试模拟人类大脑的下棋，根据以往学习到的经验判断落棋点，而不是暴力遍历所有可能性后选择最佳的下棋点，从而取胜。\n这篇文章试图介绍深度学习中很重要的技术——神经网络，并重点介绍神经网络中运用到反向传播算法。\n在笔者大学期间，除了两场围棋人机大战外，还有一场比较有意思的大型实验：大贝尔实验——关于验证基础物理量子力学中非常有名且重要的贝尔不等式的实验。所谓“遇事不决，量子力学”，这个梗就表示了量子力学中存在一些无法解释（宏观世界难以理解）的现象，若有兴趣可以再了解下。\n感知机 感知机介绍 首先让我们先来了解下神经网络中最简单也是最基础的一种结构——感知机（Perceptron），如下图：\n研究者模拟生物神经网络中的一个简单的神经元的行为——神经元兴奋过程，与之对应的感知机基础概念被提出，这里我们可以把权重当作突触、把偏置当作阈值以及把激活函数当作细胞体，当经过突触的信号总和超过了某个阈值（偏置项），细胞体就会兴奋（激活函数），产生电脉冲（输出 1），而当信号量总和未超过阈值，不产生电脉冲（输出 0）。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。感知机（单个神经元）代表了从输入空间到输出空间的如下函数： $$ y = f(\\sum_{i=1}^{n}w_ix_i + b) $$ 理想中的激活函数$f(·)$是下图 a 所示的阶跃函数，输出“1”对应神经元兴奋，输出“0”对应神经元抑制，然而阶跃函数具有不连续、不光滑等不太好的数学性质，因此实际常用 Sigmoid 函数作为激活函数，如下图 b 所示，该函数将实域范围的输出挤压到（0，1）范围输出，也被称为“挤压函数”。 对于感知机模型的理解：感知机模型的假设空间（容量）是定义在特征空间中的所有线性分类模型或线性分类器。感知机对应函数值为 0，即$w·x + b = 0$，对应于特征空间$R^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分空间中的点分别被分为正、负两类。\n感知机学习策略 感知机学习以最小化误分类点到超平面$S$的总距离为目标，因此感知机学习算法是误分类驱动的，具体使用时采用随机梯度下降法。 误分类点到超平面的总距离为 $$ -\\frac{1}{||w||}\\sum_{x_i\\in M}y_i(w·x_i + b) $$ 其中$(x_i, y_i)$为样本点，$M$为误分类点的集合，对于误分类点来说$-y_i(w·x_i + b) \u0026gt; 0$成立，且当$(w·x_i + b) \u0026gt; 0$时，$y_i = -1$，当$(w·x_i + b) \u0026lt; 0$时，$y_i = +1$。 不考虑$\\frac{1}{||w||}$，就得到感知机学习的损失函数为 $$ L(w, b) = -\\sum_{x_i\\in M}y_i(w·x_i + b) $$ 损失函数的梯度为 $$ \\nabla_w L(w, b) = - \\sum_{x_i\\in M}y_ix_i$$ $$ \\nabla_b L(w, b) = - \\sum_{x_i\\in M}y_i$$\n采用随机梯度下降法，随机选取一个误分类点$(x_i, y_i)$，对$w, b$进性更新： $$ w \\leftarrow w + \\eta y_ix_i $$ $$ b \\leftarrow b + \\eta y_i $$\n具体推导过程及算法迭代过程参见李航《统计学习方法》，代码实现参见此 github。\n感知机模型存在的问题 感知机的学习能力非常有限，与、或、非问题都是线性可分的，所以感知机可以处理，但是感知机却不能解决异或简单的非线性可分问题。 异或问题暴露出这样一个问题，即在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，对于这样的问题，一种方法，1) 可将样本从原始空间映射到一个更高维的特征空间（或者不用更高维也可以），所谓的核函数$\\phi$，使得样本在这个特征空间内线性可分，幸运的是，如果原始空间是有限维，那么一定存在一个高维特征空间使得样本线性可分。2) 另一种方法，通过增加感知机的层数，使得形成一个神经网络，其中输入层神经元接受外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。（花书理解：深层神经网络中的隐藏层即代表输入经过了一个核函数$\\phi$变换，那么以上两种方式都是在寻找一个恰当的核函数$\\phi$，神经网络的方式是通过网络自主学习学到恰当的核函数）\n神经网络 神经网络介绍 考虑到感知机模型存在的问题，即只能解决线性可分问题，我们将感知机进行连接，形成多层感知机——神经网络，如下图\n神经网络其实就是按照一定规则连接起来的多个神经元。上图展示了一个全连接(full connected, FC)神经网络，通过观察上面的图，我们可以发现它的规则包括：\n神经元按照层来布局。最左边的层叫做输入层，负责接收输入数据；最右边的层叫输出层，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做隐藏层，因为它们对于外部来说是不可见的。 同一层的神经元之间没有连接。 第N层的每个神经元和第N-1层的所有神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。 每个连接都有一个权值。 上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。\n用矩阵形式表示神经网络运算过程，如下： 第一层加权和为 $$ A^{(1)} = XW^{(1)} + B^{(1)} $$ 其中，$X$为$N\\times2$大小矩阵，$N$代表输入神经网络的数据有$N$个样本，$2$代表每个样本有两个特征值；$W$为$2\\times3$大小的矩阵代表输入层到中间层的线性变换权重矩阵；$B$为偏置项。 第一层激活后为 $$ Z^{(1)} = sigmoid(A^{(1)}) $$ 其中 sigmoid 函数作用于$A^{(1)}$中的每一个元素。将第一层的输出$Z^{(1)}$作为第二层的输入，同理得： $$ A^{(2)} = Z^{(1)}W^{(2)} + B^{(2)} $$ 注意： 因为单隐层神经网络只经过两层传播，那么第二层的输出就是网络的输出，这里一般（1）回归问题使用恒等函数作为激活函数，（2）分类问题可以使用 softmax 函数。我们把此神经网络看作是分类采用 softmax 函数： $$ y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n}exp(a_i)} $$ 那么神经网络的输出为 $$ Y = softmax(A^{(2)}) $$ 其中$Y$为$N\\times2$矩阵，形如：\n$$ \\left[ \\begin{array}{ccc} 0.2650877\u0026amp; 0.7349123\\\\ \\vdots \u0026amp; \\vdots\\\\ 0.8987211 \u0026amp; 0.1012788\\\\ \\end{array} \\right] $$\n代表第一个样本预测为第二个分类概率为 0.7349123，大于 0.5，可以认为预测为第二个分类；最后一个样本预测为第一个分类概率为 0.8987211，可以认为预测为第一个分类。（当然只有两个样本的情况下一般看作正类和负类，只需要一个输出神经元，使用 sigmoid 函数激活即可。）该模型使用 softmax 函数作为激活函数，可以推广到多分类情况下。\n神经网络学习策略 神经网络的学习中使用的指标为损失函数（loss function），即以最小化损失函数为目标，理论上，损失函数可以使用任意函数，但实际中一般用均方误差和交叉熵误差等。 均方误差为 $$ E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2 $$ 交叉熵误差为 $$ E = -\\sum_{k}t_k\\log_e{y_k} $$ 其中$y_k$表示神经网络的输出，$t_k$表示监督数据，k 表示数据的维度，如在手写数字识别例子中，$y_k$、$t_k$如下 10 个元素构成的数据：\n1 2 y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 在批量样本学习中，可以将对所有的样本的单个误差求和，然后再除以样本数 N，求得单个数据的“平均损失函数”，将它作为损失函数。 神经网络学习的策略和感知机学习策略都是一个最优化问题，神经网络学习中也是采用随机梯度下降法，与感知机学习策略更新参数类似，对于神经网络任意参数（线性权重、线性偏置等需要学习参数）更新估计式如下 $$ v \\leftarrow v + \\Delta v $$ 更新参数为线性连接权重参数时 $$ w_{hj} \\leftarrow w_{hj} + \\Delta w_{hj} $$ 采用梯度下降法，有 $$ w_{hj} \\leftarrow w_{hj} - \\eta \\frac{\\partial E}{\\partial w_{hj}} $$ 对于梯度求解，一般有解析法和数值法两种方法，随着神经网络神经元增加和层数增加，直接求上万，甚至上亿个参数的解析导数变得不现实，计算量非常大。下面将介绍两种数值方式计算梯度方法：导数定义法和反向传播法。\n梯度计算 梯度在直角坐标系下计算公式为 $$ \\nabla f(x) = (\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \u0026hellip;) $$ 梯度计算的结果是一个向量，并且可以看到计算梯度需要求函数对于各个自变量的偏导数。 对于多元函数，偏导表示自变量某个瞬间的变化量。定义为如下： $$ \\frac{\\partial f(x_1, x_2, x_3\u0026hellip;x_i\u0026hellip;)}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, x_2, x_3\u0026hellip;x_i + h, \u0026hellip;)}{h} $$ 偏导的定义和导数的定义类似，表示自变量上“微小的变化”将导致函数$f(x)$的值在多大程度上发生变化。我们可以直接用程序实现上面的计算方式，程序中无法表示 h 趋近 0，我们使用一个微小值代替，计算近似解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 数值梯度 def numerical_gradient(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和x形状相同的数组 for idx in range(x.size): tmp_val = x[idx] # 计算f(x+h) x[idx] = tmp_val + h fxh1 = f(x) # 计算f(x-h) x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原x的值 return grad 上面程序实现的偏导数方式和上面公式稍有不同，使用中心差分，误差比前向差分小，学过微积分的同学应该能够明白。**虽然上面求偏导数看着很方便很简单，但是根据上面的方式计算梯度在面对神经网络这种具有很大规模输入和很大规模参数时，计算效率相当慢。**下面将重点介绍反向传播算法计算梯度。（介绍完反向传播算法后，还会再介绍两者求解方式差异，为什么上面方式求解梯度速度在神经网络应用上效率太慢）\n计算图 为了更精确地描述反向传播算法，我们使用计算图（computational graph）语言来描述一系列操作。《深度学习》花书和斯坦福李飞飞 cs231n 课程对于计算图表示上有些区别，但都表示同一个含义，即用图的方式形式化地将操作次序表示出来，方便理解。这里我们介绍 cs231n 课程对于计算图的画法。 使用圆圈代表操作节点，指向节点的箭头代表输入变量，从节点指出的箭头代表操作完后输出的变量。上面计算图即表示了$f(x,y,z) = (x + y)z$。\n反向传播算法 简单标量反向传播算法 上面计算图真实值计算线路展示了计算的视觉化过程。前向传播从输入计算到输出（绿色），反向传播从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。 如上图计算过程，可以用下面几个中间过程合成： $$ q = x + y $$ $$ f = q \\times z $$ 在输入$x = -2, y = 5, z = -4$情况下，箭头下面红色数字代表 f 对该处变量的偏导，即 $$ \\frac{\\partial f}{\\partial f} = 1 $$ $$ \\frac{\\partial f}{\\partial q} = \\frac{\\partial (q \\times z)}{\\partial q} = z\\vert_{z=-4}=-4 $$ $$ \\frac{\\partial f}{\\partial z} = \\frac{\\partial (q \\times z)}{\\partial z} = q\\vert_{q=3}=3 $$ 根据求导的链式法则得 $$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=1 \\cdot (-4) \\cdot 1 = -4 $$ $$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=1 \\cdot (-4) \\cdot 1 = -4 $$ 结合上面计算图和求导的链式法则可以有以下结论： “$输出结果对某一变量的偏导 = 反向传播时上游传递下来的偏导数 \\times 本地偏导数（该变量输入的计算节点偏导数）$” 下面介绍稍微复杂的计算图，如下图。 利用上面得到的结论就可以得到每个遍历的偏导数，这个计算图和上面计算图区别之处在于：在乘法节点的结果输出到两个节点，分别是除法节点和减法节点，导数反向传播时，需要将上游传递的两个导数相加，即-0.25 + 1 = 0.75 这是基于下面的定理：\n定理：若函数$u=\\varphi(t), v=\\psi(t)$在点 t 可导，$z=f(u,v)$在点(u,v)处偏导连续，则复合函数$z=f(\\varphi(t),\\psi(t))$在点 t 可导，则有链式法则 $$ \\frac{\\mathrm{d} z }{\\mathrm{d} t} = \\frac{\\partial z}{\\partial u} \\cdot \\frac{\\mathrm{d} u }{\\mathrm{d} t} + \\frac{\\partial z}{\\partial v} \\cdot \\frac{\\mathrm{d} v }{\\mathrm{d} t} $$\n矩阵形式反向传播算法 下面介绍矩阵形式计算图及反向传播公式，如下图神经网络 Affine 层。 Affine 层：神经网络在正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行的仿射变换的处理称为“Affine 层”。\n神经网络中还有其他比较复杂的层，梯度推导比上面 Affine 层线性偏导数推导更为复杂，在此略过介绍及推导，但运用到的知识与上面方式类似，强烈建议自己手动推导下 Affine 层反向传播，对自己理解有帮助。\n前面介绍神经网络学习策略也是一个最优化问题，即 $$ loss \\ function = E = \\varphi(y, t) $$ 损失函数/目标函数为预测值$y$和监督数据$t$的函数，其中$y$是经过神经网络正向传播得到的预测值，所以$y$是神经网络所有参数$W$的函数，那么目标函数是关于参数$W$的函数（凸函数），即 $$ loss \\ function = \\psi(\\textbf{W}) $$ 根据梯度下降法，需要求得 $$ \\frac{\\partial L}{\\partial \\textbf{W}} $$ 这里的$L$表示$loss function$，也即是$E$。根据上面的神经网络反向传播算法逐层传播梯度，即可以高效地求解损失函数对参数的梯度，再根据随机梯度下降法，不断更新权重参数$W$，最小化目标函数，完成简单神经网络的训练学习。\n反向传播算法特别之处 现代的神经网络规模和层数不断增加，一个复杂的深度学习模型，可能每一层有成百量级神经元，有上百层深度，下图显示一个较复杂的神经网络。 如果采用导数定义方式，即$\\frac{\\partial L(w)}{\\partial w} = \\lim_{w \\to 0} \\frac{f(w + h)}{w}$求解，那么在求解每一个权重（即一条连接边）参数的偏导数时神经网络都需要向前传递一次，可以看到因为越靠后的神经元被重复计算的次数就越多，随着神经元个数和层数的增加，这将会是个指数增加的时间复杂度。然而，利用了函数求导的链式法则，从输出层到输入层逐层计算模型参数的梯度值，可以看到按照这个方向计算梯度，各个神经单元只计算了一次，没有重复计算。这个计算方向能够高效的根本原因是：在计算梯度时前面的单元是依赖后面的单元的计算，而“从后向前”的计算顺序正好“解耦”了这种依赖关系，先算后面的单元，并且记住后面单元的梯度值，计算前面单元的梯度时就可以充分利用已经计算出来的结果，避免了重复计算。\n总结与思考 如今人工智能领域中最重要的算法——反向传播算法其主要思想本质也是利用了动态规划，这种方法结合了解析法计算梯度和数值法计算梯度，利用空间换取时间，缩短求解大量参数梯度的时间，从而加速神经网络的训练。反向传播算法看上去高大上，但也不见得是用了多高级的方法和高深的知识。\n参考资料：\n维基百度-感知器\n维基百度-人工神经网络\n计算机的潜意识\n《Deep Learning form Scratch》\n《深度学习》\n周志华《机器学习》\n《深度学习[花书]》\n李航《统计学习方法》\n零基础入门深度学习(3) - 神经网络和反向传播算法(牛逼)\n","date":"2020-06-13T13:47:01Z","permalink":"https://qiyueliuhuo.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","title":"神经网络之反向传播算法"}]